#
# This script is Copyright (C) 2004-2020 and is owned by Tenable, Inc. or an Affiliate thereof.
#
# This script is released under the Tenable Subscription License and
# may not be used from within scripts released under another license
# without authorization from Tenable, Inc.
#
# See the following licenses for details:
#
# http://static.tenable.com/prod_docs/Nessus_6_SLA_and_Subscription_Agreement.pdf
#
# @PROFESSIONALFEED@
# $Revision: 1.2 $
# $Date: 2020/07/14 $
#
# description : This document implements the security configuration as recommended by the
#               CIS Kubernetes Benchmark v1.4.1
#
#               https://workbench.cisecurity.org/files/2421
#
#<ui_metadata>
#<display_name>CIS Kubernetes 1.13 Benchmark v1.4.1 L1</display_name>
#<spec>
#  <type>CIS</type>
#  <name>Kubernetes 1.13 L1</name>
#  <version>1.4.1</version>
#  <link>https://workbench.cisecurity.org/files/2421</link>
#</spec>
#<labels>kubernetes,kubernetes_1.13,agent,unix</labels>
#<benchmark_refs>LEVEL,CSCv6,CSCv7</benchmark_refs>
#<variables>
#  <variable>
#    <name>ADMIN_KUBECONFIG_FILE</name>
#    <default>/etc/kubernetes/admin.conf</default>
#    <description>Administrator Kubeconfig File</description>
#    <info>The administrator kubeconfig file defining various settings for the administration of the cluster.</info>
#  </variable>
#  <variable>
#    <name>APISERVER_FILE</name>
#    <default>/etc/kubernetes/manifests/kube-apiserver.yaml</default>
#    <description>API Server Config File</description>
#    <info>The apiserver file controls various parameters that set the behavior of the API server.</info>
#  </variable>
#  <variable>
#    <name>AUDIT_LOG_MAXAGE</name>
#    <default>30</default>
#    <description>Audit Log Maximum Age</description>
#    <info>The maximum number of days to retain old audit log files based on the timestamp encoded in their filename.</info>
#  </variable>
#  <variable>
#    <name>AUDIT_LOG_MAXBACKUP</name>
#    <default>10</default>
#    <description>Audit Log Maximum Backups</description>
#    <info>The maximum number of old audit log files to retain.</info>
#  </variable>
#  <variable>
#    <name>AUDIT_LOG_MAXSIZE</name>
#    <default>100</default>
#    <description>Audit Log Maximum Size</description>
#    <info>The maximum size in megabytes of the audit log file before it gets rotated.</info>
#  </variable>
#  <variable>
#    <name>AUDIT_LOG_PATH</name>
#    <default>/var/log/apiserver/audit.log</default>
#    <description>Audit Log Path</description>
#    <info>All requests coming to the apiserver will be logged to this file.  '-' means standard out.</info>
#  </variable>
#  <variable>
#    <name>AUDIT_POLICY_FILE</name>
#    <default>/etc/kubernetes/audit-policy.yaml</default>
#    <description>Audit Policy File</description>
#    <info>The audit policy file contains rules which define what type of data is logged.</info>
#  </variable>
#  <variable>
#    <name>CERT_FILE</name>
#    <default>/etc/kubernetes/pki/etcd.crt</default>
#    <description>Etcd Certificate File</description>
#    <info>Certificate used for SSL/TLS connections to etcd.</info>
#  </variable>
#  <variable>
#    <name>CLIENT_CA_FILE</name>
#    <default>/etc/kubernetes/pki/ca.crt</default>
#    <description>Client CA File</description>
#    <info>Any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</info>
#  </variable>
#  <variable>
#    <name>CONFIG_FILE</name>
#    <default>/etc/kubernetes/kubelet.conf</default>
#    <description>Worker Node Config File</description>
#    <info>The config file controls various parameters that set the behavior of various components of the worker node.</info>
#  </variable>
#  <variable>
#    <name>CONTROLLER_MANAGER_KUBECONFIG_FILE</name>
#    <default>/etc/kubernetes/controller-manager.conf</default>
#    <description>Controller Manager Kubeconfig File</description>
#    <info>The kubeconfig file for the Controller Manager.</info>
#  </variable>
#  <variable>
#    <name>CONTROLLER_MANAGER_CONFIG_FILE</name>
#    <default>/etc/kubernetes/manifests/kube-controller-manager.yaml</default>
#    <description>Controller Manager Config File</description>
#    <info>The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager.</info>
#  </variable>
#  <variable>
#    <name>ETCD_CAFILE</name>
#    <default>/etc/kubernetes/pki/etcd-ca.crt</default>
#    <description>Etcd Certificate Authority File</description>
#    <info>SSL Certificate Authority file used to secure etcd communication.</info>
#  </variable>
#  <variable>
#    <name>ETCD_CERTFILE</name>
#    <default>/etc/kubernetes/pki/etcd.crt</default>
#    <description>Etcd Certification File</description>
#    <info>SSL certification file used to secure etcd communication.</info>
#  </variable>
#  <variable>
#    <name>ETCD_CONF_FILE</name>
#    <default>/etc/kubernetes/manifests/etcd.yaml</default>
#    <description>Etcd Configuration File</description>
#    <info>The etcd.conf file controls various parameters that set the behavior of the etcd service in the master node.</info>
#  </variable>
#  <variable>
#    <name>ETCD_DATA_DIR</name>
#    <default>/var/lib/etcd</default>
#    <description>Etcd Data Directory</description>
#    <info>False</info>
#  </variable>
#  <variable>
#    <name>ETCD_KEYFILE</name>
#    <default>/etc/kubernetes/pki/etcd.key</default>
#    <description>Etcd Key File</description>
#    <info>SSL key file used to secure etcd communication.</info>
#  </variable>
#  <variable>
#    <name>ENCRYPTION_PROVIDER_CONFIG</name>
#    <default>/etc/kubernetes/encryption</default>
#    <description>Encryption Provider Config File</description>
#    <info>The file containing configuration for encryption providers to be used for storing secrets in etcd</info>
#  </variable>
#  <variable>
#    <name>FLANNELD_FILE</name>
#    <default>/etc/cni/net.d/10-flannel.conf</default>
#    <description>Flanneld Configuration File</description>
#    <info>False</info>
#  </variable>
#  <variable>
#    <name>KEY_FILE</name>
#    <default>/etc/kubernetes/pki/etcd.key</default>
#    <description>Key File</description>
#    <info>Key for the certificate</info>
#  </variable>
#  <variable>
#    <name>KUBECONFIG</name>
#    <default>/etc/kubernetes/admin.conf</default>
#    <description>Path to the kubectl config</description>
#    <info>The kubeconfig file controls various parameters for connecting to the API server.</info>
#  </variable>
#  <variable>
#    <name>KUBECTL_PATH</name>
#    <default>/usr/bin</default>
#    <description>Path to the kubectl binary</description>
#    <info>This is the directory where the kubectl binary is located.</info>
#  </variable>
#  <variable>
#    <name>KUBELET_CERTIFICATE_AUTHORITY</name>
#    <default>/etc/kubernetes/pki/apiserver-kubelet-ca.crt</default>
#    <description>Kubelet Certification Authority File</description>
#    <info>Path to a cert file for the certificate authority.</info>
#  </variable>
#  <variable>
#    <name>KUBELET_CLIENT_CERTIFICATE</name>
#    <default>/etc/kubernetes/pki/apiserver-kubelet-client.crt</default>
#    <description>Kubelet Client Certificate File</description>
#    <info>Path to a client cert file for TLS.</info>
#  </variable>
#  <variable>
#    <name>KUBELET_CLIENT_KEY</name>
#    <default>/etc/kubernetes/pki/apiserver-kubelet-client.key</default>
#    <description>Kubelet Client Key File</description>
#    <info>Path to a client key file for TLS.</info>
#  </variable>
#  <variable>
#    <name>KUBELET_CONFIG_FILE</name>
#    <default>/var/lib/kubelet/config.yaml</default>
#    <description>Kubelet Config File</description>
#    <info>The kubelet config file controls various parameters that set the behavior of the kubelet service.</info>
#  </variable>
#  <variable>
#    <name>KUBELET_FILE</name>
#    <default>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</default>
#    <description>Kubelet Service Config File</description>
#    <info>The kubelet file controls various parameters that set the behavior of the kubelet service in the worker node.</info>
#  </variable>
#  <variable>
#    <name>PEER_CERT_FILE</name>
#    <default>/etc/kubernetes/pki/etcd/peer.crt</default>
#    <description>Peer Certificate File</description>
#    <info>Path to the peer server TLS cert file.</info>
#  </variable>
#  <variable>
#    <name>PEER_KEY_FILE</name>
#    <default>/etc/kubernetes/pki/etcd/peer.key</default>
#    <description>Peer Key File</description>
#    <info>Path to the peer server TLS key file.</info>
#  </variable>
#  <variable>
#    <name>PKI_DIRECTORY</name>
#    <default>/etc/kubernetes/pki</default>
#    <description>PKI Directory</description>
#    <info>Directory containing PKI certificates and keys.</info>
#  </variable>
#  <variable>
#    <name>PROXY_FILE</name>
#    <default>/etc/kubernetes/proxy</default>
#    <description>Proxy File</description>
#    <info>The proxy file controls various parameters that set the behavior of the kube-proxy service in the worker node.</info>
#  </variable>
#  <variable>
#    <name>REQUEST_TIMEOUT</name>
#    <default>300</default>
#    <description>API Server Request Timeout</description>
#    <info>Duration in seconds before API server requests time out.</info>
#  </variable>
#  <variable>
#    <name>ROOT_CA_FILE</name>
#    <default>/etc/kubernetes/pki/ca.crt</default>
#    <description>Root Certificate Authority File</description>
#    <info>The root certificate authority will be included in service account's token secret.</info>
#  </variable>
#  <variable>
#    <name>SCHEDULER_FILE</name>
#    <default>/etc/kubernetes/manifests/kube-scheduler.yaml</default>
#    <description>Scheduler Config File</description>
#    <info>The scheduler file controls various parameters that set the behavior of the kube-scheduler service in the master node.</info>
#  </variable>
#  <variable>
#    <name>SCHEDULER_KUBECONFIG_FILE</name>
#    <default>/etc/kubernetes/scheduler.conf</default>
#    <description>Scheduler Kubeconfig File</description>
#    <info>The kubeconfig file for the Scheduler.</info>
#  </variable>
#  <variable>
#    <name>SERVICE_ACCOUNT_KEY_FILE</name>
#    <default>/etc/kubernetes/pki/sa.pub</default>
#    <description>Service Accout Key File</description>
#    <info>File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens.</info>
#  </variable>
#  <variable>
#    <name>SERVICE_ACCOUNT_PRIVATE_KEY_FILE</name>
#    <default>/etc/kubernetes/pki/sa.key</default>
#    <description>Service Account Private Key File</description>
#    <info>Filename containing a PEM-encoded private RSA or ECDSA key used to sign service account tokens.</info>
#  </variable>
#  <variable>
#    <name>TERMINATED_POD_GC_THRESHOLD</name>
#    <default>12500</default>
#    <description>Terminated Pod Garbage Collection Threshold</description>
#    <info>Number of terminated pods that can exist before the terminated pod garbage collector starts deleting terminated pods.</info>
#  </variable>
#  <variable>
#    <name>TLS_CERT_FILE</name>
#    <default>/etc/kubernetes/pki/apiserver.crt</default>
#    <description>TLS Certificate File</description>
#    <info>File containing the default x509 Certificate for HTTPS.</info>
#  </variable>
#  <variable>
#    <name>TLS_PRIVATE_KEY_FILE</name>
#    <default>/etc/kubernetes/pki/apiserver.key</default>
#    <description>TLS Private Key File</description>
#    <info>File containing the default x509 private key matching --tls-cert-file.</info>
#  </variable>
#</variables>
#</ui_metadata>

<check_type:"Unix">

<custom_item>
  system      : "Linux"
  type        : PROCESS_CHECK
  description : "CIS_Kubernetes_v1.4.1_Level_1.audit from CIS Kubernetes Benchmark v1.4.1"
  info        : "NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
  name        : "kubelet"
  status      : ON
  severity    : MEDIUM
  reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSF|PR.IP-1,ITSG-33|CM-6,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
</custom_item>

<if>
  <condition type:"AND">
    <custom_item>
      system      : "Linux"
      type        : PROCESS_CHECK
      description : "Check if kubelet is running"
      name        : "kubelet"
      status      : ON
    </custom_item>

    <custom_item>
      type        : CMD_EXEC
      description : "Check if this is a Docker Vessel/Host"
      cmd         : "/usr/bin/docker info"
      expect      : "Containers"
    </custom_item>
  </condition>

  <then>
    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : PROCESS_CHECK
          description : "Check if API Server is running"
          name        : "kube-apiserver"
          status      : ON
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.1 Ensure that the --anonymous-auth argument is set to false"
          info          : "Disable anonymous requests to the API server.

Rationale:

When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests.

If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the below parameter.

--anonymous-auth=false"
          reference     : "800-171|3.5.1,800-53|IA-2,CN-L3|7.1.3.1(a),CN-L3|7.1.3.1(e),CN-L3|8.1.4.1(a),CN-L3|8.1.4.2(a),CN-L3|8.5.4.1(a),CSCv6|14,CSF|PR.AC-1,ITSG-33|IA-2,LEVEL|1S,NESA|T2.3.8,NESA|T5.3.1,NESA|T5.4.2,NESA|T5.5.1,NESA|T5.5.2,NESA|T5.5.3,NIAv2|AM14b,NIAv2|AM2,NIAv2|AM8,TBA-FIISB|35.1,TBA-FIISB|36.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--anonymous-auth=false"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.2 Ensure that the --basic-auth-file argument is not set"
          info          : "Do not use basic authentication.

Rationale:

Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used."
          solution      : "Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and remove the '--basic-auth-file=' parameter."
          reference     : "800-171|3.5.10,800-53|IA-5(1),CSCv6|16.14,CSCv7|16.4,CSF|PR.AC-1,ITSG-33|IA-5(1),LEVEL|1S,NESA|T5.2.3,NIAv2|CY6,SWIFT-CSCv1|4.1,TBA-FIISB|26.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "^((?!--basic-auth-file).)*$"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.3 Ensure that the --insecure-allow-any-token argument is not set"
          info          : "Do not allow any insecure tokens

Rationale:

Accepting insecure tokens would allow any token without actually authenticating anything. User information is parsed from the token and connections are allowed."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and remove the '--insecure-allow-any-token' parameter."
          reference     : "800-171|3.5.1,800-53|IA-2,CN-L3|7.1.3.1(a),CN-L3|7.1.3.1(e),CN-L3|8.1.4.1(a),CN-L3|8.1.4.2(a),CN-L3|8.5.4.1(a),CSCv6|16,CSF|PR.AC-1,ITSG-33|IA-2,LEVEL|1S,NESA|T2.3.8,NESA|T5.3.1,NESA|T5.4.2,NESA|T5.5.1,NESA|T5.5.2,NESA|T5.5.3,NIAv2|AM14b,NIAv2|AM2,NIAv2|AM8,TBA-FIISB|35.1,TBA-FIISB|36.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "^((?!--insecure-allow-any-token).)*$"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.4 Ensure that the --kubelet-https argument is set to true"
          info          : "Use https for kubelet connections.

Rationale:

Connections from apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and remove the '--kubelet-https' parameter."
          reference     : "800-171|3.13.8,800-53|SC-8(1),CSCv6|14.2,CSCv7|14.4,CSF|PR.DS-2,CSF|PR.DS-5,ISO/IEC-27001|A.10.1.1,ISO/IEC-27001|A.13.2.3,ITSG-33|SC-8(1),LEVEL|1S,NESA|T7.4.1,NIAv2|NS5d,NIAv2|NS6b,SWIFT-CSCv1|2.1,TBA-FIISB|29.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "(--kubelet-https=true|^((?!--kubelet-https).)*$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.5 Ensure that the --insecure-bind-address argument is not set"
          info          : "Do not bind the insecure API service.

Rationale:

If you bind the apiserver to an insecure address, basically anyone who could connect to it over the insecure port, would have unauthenticated and unencrypted access to your master node. The apiserver doesn't do any authentication checking for insecure binds and traffic to the Insecure API port is not encrpyted, allowing attackers to potentially read sensitive data in transit."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and remove the '--insecure-bind-address' parameter."
          reference     : "800-171|3.4.6,800-171|3.4.7,800-53|CM-7,CN-L3|7.1.3.5(c),CN-L3|7.1.3.7(d),CN-L3|8.1.4.4(b),CSCv6|9.1,CSCv7|9.2,CSF|PR.IP-1,CSF|PR.PT-3,ITSG-33|CM-7,LEVEL|1S,NIAv2|SS13b,NIAv2|SS14a,NIAv2|SS14c,NIAv2|SS15a,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "^((?!--insecure-bind-address).)*$"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.6 Ensure that the --insecure-port argument is set to 0"
          info          : "Do not bind to insecure port.

Rationale:

Setting up the apiserver to serve on an insecure port would allow unauthenticated and unencrypted access to your master node. This would allow attackers who could access this port, to easily take control of the cluster."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the below parameter.

--insecure-port=0"
          reference     : "800-171|3.4.6,800-171|3.4.7,800-53|CM-7,CN-L3|7.1.3.5(c),CN-L3|7.1.3.7(d),CN-L3|8.1.4.4(b),CSCv6|9.1,CSCv7|9.2,CSF|PR.IP-1,CSF|PR.PT-3,ITSG-33|CM-7,LEVEL|1S,NIAv2|SS13b,NIAv2|SS14a,NIAv2|SS14c,NIAv2|SS15a,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--insecure-port=0([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.7 Ensure that the --secure-port argument is not set to 0"
          info          : "Do not disable the secure port.

Rationale:

The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and either remove the '--secure-port' parameter or set it to a different (non-zero) desired port."
          reference     : "800-171|3.13.8,800-53|SC-8(1),CSCv6|14.2,CSCv7|14.4,CSF|PR.DS-2,CSF|PR.DS-5,ISO/IEC-27001|A.10.1.1,ISO/IEC-27001|A.13.2.3,ITSG-33|SC-8(1),LEVEL|1S,NESA|T7.4.1,NIAv2|NS5d,NIAv2|NS6b,SWIFT-CSCv1|2.1,TBA-FIISB|29.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "(--secure-port=[1-9]|^((?!--secure-port).)*$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.8 Ensure that the --profiling argument is set to false"
          info          : "Disable profiling, if not needed.

Rationale:

Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the below parameter.

--profiling=false"
          reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|14,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--profiling=false"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.9 Ensure that the --repair-malformed-updates argument is set to false"
          info          : "Disable fixing of malformed updates.

Rationale:

The API Server will potentially attempt to fix the update requests to pass the validation even if the requests are malformed. Malformed requests are one of the potential ways to interact with a service without legitimate information. Such requests could potentially be used to sabotage API Server responses."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the below parameter.

--repair-malformed-updates=false"
          reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|14,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--repair-malformed-updates=false"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.10 Ensure that the admission control plugin AlwaysAdmit is not set"
          info          : "Do not allow all requests.

Rationale:

Setting admission control plugin 'AlwaysAdmit' allows all requests and do not filter any requests."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--enable-admission-plugins' parameter to a value that does not include 'AlwaysAdmit'."
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--enable-admission-plugins=((?!AlwaysAdmit).)*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.11 Ensure that the admission control plugin AlwaysPullImages is set"
          info          : "Always pull images.

Rationale:

Setting admission control policy to 'AlwaysPullImages' forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the images name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--enable-admission-plugins' parameter to include 'AlwaysPullImages'.

--enable-admission-plugins=...,AlwaysPullImages,..."
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14.4,CSCv7|14.6,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--enable-admission-plugins=[A-z,]*AlwaysPullImages[A-z,]*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.13 Ensure that the admission control plugin SecurityContextDeny is set"
          info          : "Restrict pod level SecurityContext customization. Instead of using a customized SecurityContext for your pods, use a Pod Security Policy (PSP), which is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access.

Rationale:

Setting admission control policy to 'SecurityContextDeny' denies the pod level SecurityContext customization. Any attempts to customize the SecurityContexts that are not explicitly defined in the Pod Security Policy (PSP) are blocked. This ensures that all the pods adhere to the PSP defined by your organization and you have a uniform pod level security posture."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--enable-admission-plugins' parameter to include 'SecurityContextDeny'.

--enable-admission-plugins=...,SecurityContextDeny,..."
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|5.1,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--enable-admission-plugins=[A-z,]*SecurityContextDeny[A-z,]*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.14 Ensure that the admission control plugin NamespaceLifecycle is set"
          info          : "Reject creating objects in a namespace that is undergoing termination.

Rationale:

Setting admission control policy to 'NamespaceLifecycle' ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--disable-admission-plugins' parameter to ensure it does not include 'NamespaceLifecycle'."
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "^((?!--disable-admission-plugins=[A-z,]*NamespaceLifecycle[\\s,]).)*$"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.15 Ensure that the --audit-log-path argument is set as appropriate"
          info          : "Enable auditing on the Kubernetes API Server and set the desired audit log path as appropriate.

Rationale:

Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--audit-log-path' parameter to a suitable path and file where you would like audit logs to be written, for example:

--audit-log-path=/var/log/apiserver/audit.log"
          reference     : "800-171|3.3.1,800-171|3.3.2,800-53|AU-12,CN-L3|7.1.3.3(a),CN-L3|7.1.3.3(b),CN-L3|7.1.3.3(c),CN-L3|8.1.3.5(a),CN-L3|8.1.3.5(b),CN-L3|8.1.4.3(a),CSCv6|6.2,CSCv7|6.2,CSCv7|6.3,CSF|DE.CM-1,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.PT-1,ISO/IEC-27001|A.12.4.1,ITSG-33|AU-12,LEVEL|1S,NESA|T3.6.2,NESA|T3.6.5,NESA|T3.6.6,NIAv2|SM8,SWIFT-CSCv1|6.4,TBA-FIISB|45.1.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @AUDIT_LOG_PATH@ replaced with "/var/log/apiserver/audit.log" in field "expect".
          expect        : "--audit-log-path=/var/log/apiserver/audit.log([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.16 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate"
          info          : "Retain the logs for at least 30 days or as appropriate.

Rationale:

Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--audit-log-maxage' parameter to 30 or as an appropriate number of days:

--audit-log-maxage=30"
          reference     : "800-53|AU-4,CSCv6|6.3,CSCv7|6.4,CSF|PR.DS-4,CSF|PR.PT-1,ITSG-33|AU-4,LEVEL|1S,NESA|T3.3.1,NESA|T3.6.2"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @AUDIT_LOG_MAXAGE@ replaced with "30" in field "expect".
          expect        : "--audit-log-maxage=30([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.17 Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate"
          info          : "Retain 10 or an appropriate number of old log files.

Rationale:

Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--audit-log-maxbackup' parameter to 10 or to an appropriate value.

--audit-log-maxbackup=10"
          reference     : "800-53|AU-4,CSCv6|6.3,CSCv7|6.4,CSF|PR.DS-4,CSF|PR.PT-1,ITSG-33|AU-4,LEVEL|1S,NESA|T3.3.1,NESA|T3.6.2"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @AUDIT_LOG_MAXBACKUP@ replaced with "10" in field "expect".
          expect        : "--audit-log-maxbackup=10([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.18 Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate"
          info          : "Rotate log files on reaching 100 MB or as appropriate.

Rationale:

Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--audit-log-maxsize' parameter to an appropriate size in MB. For example, to set it as 100 MB:

--audit-log-maxsize=100"
          reference     : "800-53|AU-4,CSCv6|6.3,CSCv7|6.4,CSF|PR.DS-4,CSF|PR.PT-1,ITSG-33|AU-4,LEVEL|1S,NESA|T3.3.1,NESA|T3.6.2"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @AUDIT_LOG_MAXSIZE@ replaced with "100" in field "expect".
          expect        : "--audit-log-maxsize=100([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.19 Ensure that the --authorization-mode argument is not set to AlwaysAllow"
          info          : "Do not always authorize all requests.

Rationale:

The API Server, can be configured to allow all requests. This mode should not be used on any production cluster."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--authorization-mode' parameter to values other than 'AlwaysAllow'. One such example could be as below.

--authorization-mode=RBAC"
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|9.1,CSCv7|9.2,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--authorization-mode=((?!AlwaysAllow).)*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.20 Ensure that the --token-auth-file parameter is not set"
          info          : "Do not use token based authentication.

Rationale:

The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication."
          solution      : "Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and remove the '--token-auth-file=' parameter."
          reference     : "800-171|3.5.10,800-53|IA-5(1),CSCv6|16.14,CSCv7|16.4,CSF|PR.AC-1,ITSG-33|IA-5(1),LEVEL|1S,NESA|T5.2.3,NIAv2|CY6,SWIFT-CSCv1|4.1,TBA-FIISB|26.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "^((?!--token-auth-file).)*$"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.21 Ensure that the --kubelet-certificate-authority argument is set as appropriate"
          info          : "Verify kubelet's certificate before establishing connection.

Rationale:

The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelets port-forwarding functionality. These connections terminate at the kubelets HTTPS endpoint. By default, the apiserver does not verify the kubelets serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks."
          solution      : "Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--kubelet-certificate-authority' parameter to the path to the cert file for the certificate authority.

--kubelet-certificate-authority="
          reference     : "800-53|IA-5(2),CSCv6|3.4,CSCv7|4.5,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @KUBELET_CERTIFICATE_AUTHORITY@ replaced with "/etc/kubernetes/pki/apiserver-kubelet-ca.crt" in field "expect".
          expect        : "--kubelet-certificate-authority=/etc/kubernetes/pki/apiserver-kubelet-ca.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.22 Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate - certificate"
          info          : "Enable certificate based kubelet authentication.

Rationale:

The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate-based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests."
          solution      : "Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the kubelet client certificate and key parameters as below.

--kubelet-client-certificate=
--kubelet-client-key="
          reference     : "800-53|IA-5(2),CSCv6|3.4,CSCv7|4.5,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @KUBELET_CLIENT_CERTIFICATE@ replaced with "/etc/kubernetes/pki/apiserver-kubelet-client.crt" in field "expect".
          expect        : "--kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.22 Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate - key"
          info          : "Enable certificate based kubelet authentication.

Rationale:

The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate-based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests."
          solution      : "Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the kubelet client certificate and key parameters as below.

--kubelet-client-certificate=
--kubelet-client-key="
          reference     : "800-53|IA-5(2),CSCv6|3.4,CSCv7|4.5,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @KUBELET_CLIENT_KEY@ replaced with "/etc/kubernetes/pki/apiserver-kubelet-client.key" in field "expect".
          expect        : "--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.23 Ensure that the --service-account-lookup argument is set to true"
          info          : "Validate service account before validating token.

Rationale:

If '--service-account-lookup' is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the below parameter.

--service-account-lookup=true

Alternatively, you can delete the '--service-account-lookup' parameter from this file so that the default takes effect."
          reference     : "800-53|IA-5(13),CSCv6|16,CSF|PR.AC-1,LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--service-account-lookup=true"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.24 Ensure that the admission control plugin PodSecurityPolicy is set"
          info          : "Reject creating pods that do not match Pod Security Policies.

Rationale:

A Pod Security Policy is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access. The 'PodSecurityPolicy' objects define a set of conditions that a pod must run with in order to be accepted into the system. Pod Security Policies are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions.

**Note:** When the PodSecurityPolicy admission plugin is in use, there needs to be at least one PodSecurityPolicy in place for ANY pods to be admitted. See section 1.7 for recommendations on PodSecurityPolicy settings."
          solution      : "Follow the documentation and create Pod Security Policy objects as per your environment. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--enable-admission-plugins' parameter to a value that includes 'PodSecurityPolicy':

--enable-admission-plugins=...,PodSecurityPolicy,...

Then restart the API Server."
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--enable-admission-plugins=[A-z,]*PodSecurityPolicy[A-z,]*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.25 Ensure that the --service-account-key-file argument is set as appropriate"
          info          : "Explicitly set a service account public key file for service accounts on the apiserver.

Rationale:

By default, if no '--service-account-key-file' is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with '--service-account-key-file'."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--service-account-key-file' parameter to the public key file for service accounts:

--service-account-key-file="
          reference     : "800-53|IA-5(2),CSCv6|3,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @SERVICE_ACCOUNT_KEY_FILE@ replaced with "/etc/kubernetes/pki/sa.pub" in field "expect".
          expect        : "--service-account-key-file=/etc/kubernetes/pki/sa.pub([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.26 Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate - certfile"
          info          : "etcd should be configured to make use of TLS encryption for client connections.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key."
          solution      : "Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the etcd certificate and key file parameters.

--etcd-certfile=
--etcd-keyfile="
          reference     : "800-53|IA-5(2),CSCv6|9,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @ETCD_CERTFILE@ replaced with "/etc/kubernetes/pki/etcd.crt" in field "expect".
          expect        : "--etcd-certfile=/etc/kubernetes/pki/etcd.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.26 Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate - keyfile"
          info          : "etcd should be configured to make use of TLS encryption for client connections.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key."
          solution      : "Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the etcd certificate and key file parameters.

--etcd-certfile=
--etcd-keyfile="
          reference     : "800-53|IA-5(2),CSCv6|9,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @ETCD_KEYFILE@ replaced with "/etc/kubernetes/pki/etcd.key" in field "expect".
          expect        : "--etcd-keyfile=/etc/kubernetes/pki/etcd.key([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.27 Ensure that the admission control plugin ServiceAccount is set"
          info          : "Automate service accounts management.

Rationale:

When you create a pod, if you do not specify a service account, it is automatically assigned the 'default' service account in the same namespace. You should create your own service account and let the API server manage its security tokens."
          solution      : "Follow the documentation and create 'ServiceAccount' objects as per your environment. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and ensure that the '--disable-admission-plugins' parameter is set to a value that does not include 'ServiceAccount'."
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|16,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--enable-admission-plugins=((?!ServiceAccount).)*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.28 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - cert"
          info          : "Setup TLS connection on the API server.

Rationale:

API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic."
          solution      : "Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the TLS certificate and private key file parameters.

--tls-cert-file=
--tls-private-key-file="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @TLS_CERT_FILE@ replaced with "/etc/kubernetes/pki/apiserver.crt" in field "expect".
          expect        : "--tls-cert-file=/etc/kubernetes/pki/apiserver.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.28 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - key"
          info          : "Setup TLS connection on the API server.

Rationale:

API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic."
          solution      : "Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the TLS certificate and private key file parameters.

--tls-cert-file=
--tls-private-key-file="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @TLS_PRIVATE_KEY_FILE@ replaced with "/etc/kubernetes/pki/apiserver.key" in field "expect".
          expect        : "--tls-private-key-file=/etc/kubernetes/pki/apiserver.key([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.29 Ensure that the --client-ca-file argument is set as appropriate"
          info          : "Setup TLS connection on the API server.

Rationale:

API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If '--client-ca-file' argument is set, any request presenting a client certificate signed by one of the authorities in the 'client-ca-file' is authenticated with an identity corresponding to the CommonName of the client certificate."
          solution      : "Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the client certificate authority file.

--client-ca-file="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @CLIENT_CA_FILE@ replaced with "/etc/kubernetes/pki/ca.crt" in field "expect".
          expect        : "--client-ca-file=/etc/kubernetes/pki/ca.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.30 Ensure that the API Server only makes use of Strong Cryptographic Ciphers"
          info          : "Ensure that the API server is configured to only use strong cryptographic ciphers.

Rationale:

TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided."
          solution      : "Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the below parameter.

--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256"
          reference     : "800-171|3.13.11,800-53|SC-13,CSCv6|3.4,CSCv7|4.5,CSF|PR.DS-5,ISO/IEC-27001|A.10.1.1,ITSG-33|SC-13,LEVEL|1S,NESA|M5.2.6,NESA|T7.4.1,NIAv2|CY3,NIAv2|CY4,NIAv2|CY5b,NIAv2|CY5c,NIAv2|CY5d,NIAv2|CY7,NIAv2|NS5e"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--tls-cipher-suites=((TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256|TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256|TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305|TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384|TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305|TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384|TLS_RSA_WITH_AES_256_GCM_SHA384|TLS_RSA_WITH_AES_128_GCM_SHA256)[,]?)+([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.31 Ensure that the --etcd-cafile argument is set as appropriate"
          info          : "etcd should be configured to make use of TLS encryption for client connections.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file."
          solution      : "Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the etcd certificate authority file parameter.

--etcd-cafile="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @ETCD_CAFILE@ replaced with "/etc/kubernetes/pki/etcd-ca.crt" in field "expect".
          expect        : "--etcd-cafile=/etc/kubernetes/pki/etcd-ca.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.32 Ensure that the --authorization-mode argument includes Node"
          info          : "Restrict kubelet nodes to reading only objects associated with them.

Rationale:

The 'Node' authorization mode only allows kubelets to read 'Secret', 'ConfigMap', 'PersistentVolume', and 'PersistentVolumeClaim' objects associated with their nodes."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--authorization-mode' parameter to a value that includes 'Node'.

--authorization-mode=Node,RBAC"
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|9.1,CSCv7|9.2,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--authorization-mode=[A-z,]*Node[A-z,]*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.33 Ensure that the admission control plugin NodeRestriction is set"
          info          : "Limit the 'Node' and 'Pod' objects that a kubelet could modify.

Rationale:

Using the 'NodeRestriction' plug-in ensures that the kubelet is restricted to the 'Node' and 'Pod' objects that it could modify as defined. Such kubelets will only be allowed to modify their own 'Node' API object, and only modify 'Pod' API objects that are bound to their node."
          solution      : "Follow the Kubernetes documentation and configure 'NodeRestriction' plug-in on kubelets. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--enable-admission-plugins' parameter to a value that includes 'NodeRestriction'.

--enable-admission-plugins=...,NodeRestriction,..."
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--enable-admission-plugins=[A-z,]*NodeRestriction[A-z,]*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.34 Ensure that the --encryption-provider-config argument is set as appropriate"
          info          : "Encrypt etcd key-value store.

Rationale:

etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures."
          solution      : "Follow the Kubernetes documentation and configure a 'EncryptionConfig' file. Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--encryption-provider-config' parameter to the path of that file:

--encryption-provider-config=</path/to/EncryptionConfig/File>

Impact:

None

Default Value:

By default, '--encryption-provider-config' is not set."
          reference     : "800-171|3.13.16,800-53|SC-28(1),CSCv6|14.5,CSCv7|14.8,CSF|PR.DS-1,ITSG-33|SC-28(1),LEVEL|1S,TBA-FIISB|28.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @ENCRYPTION_PROVIDER_CONFIG@ replaced with "/etc/kubernetes/encryption" in field "expect".
          expect        : "--encryption-provider-config=/etc/kubernetes/encryption([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system      : "Linux"
          type        : FILE_CONTENT_CHECK_NOT
          description : "1.1.35 Ensure that the encryption provider is set to aescbc"
          info        : "Use 'aescbc' encryption provider.

Rationale:

'aescbc' is currently the strongest encryption provider, It should be preferred over other providers."
          solution    : "Follow the Kubernetes documentation and configure a 'EncryptionConfig' file. In this file, choose 'aescbc' as the encryption provider.

For example,

kind: EncryptionConfig
apiVersion: v1
resources:
 - resources:
 - secrets
 providers:
 - aescbc:
 keys:
 - name: key1
 secret:"
          reference   : "800-171|3.13.16,800-53|SC-28(1),CSCv6|14.5,CSCv7|14.8,CSF|PR.DS-1,ITSG-33|SC-28(1),LEVEL|1S,TBA-FIISB|28.1"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @ENCRYPTION_PROVIDER_CONFIG@ replaced with "/etc/kubernetes/encryption" in field "file".
          file        : "/etc/kubernetes/encryption"
          regex       : "^[\\s]*-[\\s]*(secretbox|aesgcm)[\\s]*:"
          expect      : "^[\\s]*-[\\s]*(secretbox|aesgcm)[\\s]*:"
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.36 Ensure that the admission control plugin EventRateLimit is set"
          info          : "Limit the rate at which the API server accepts requests.

Rationale:

Using 'EventRateLimit' admission control enforces a limit on the number of events that the API Server will accept in a given time slice. In a large multi-tenant cluster, there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept.

Note: This is an Alpha feature in the Kubernetes 1.11 release."
          solution      : "Follow the Kubernetes documentation and set the desired limits in a configuration file.

Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' and set the below parameters.

--enable-admission-plugins=...,EventRateLimit,...
--admission-control-config-file="
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|8.4,CSCv7|8.3,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--enable-admission-plugins=[A-z,]*EventRateLimit[A-z,]*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.37 Ensure that the AdvancedAuditing argument is not set to false - audit-policy-file"
          info          : "Do not disable advanced auditing.

Rationale:

'AdvancedAuditing' enables a much more general API auditing pipeline, which includes support for pluggable output backends and an audit policy specifying how different requests should be audited. Additionally, this enables auditing of failed authentication, authorization and login attempts which could prove crucial for protecting your production clusters. It is thus recommended not to disable advanced auditing."
          solution      : "Follow the Kubernetes documentation and set the desired audit policy in the '/etc/kubernetes/audit-policy.yaml' file.

Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' and set the below parameters.

--audit-policy-file=/etc/kubernetes/audit-policy.yaml

In the same API server pod specification file ensure that if the '--feature-gates' argument is present, it does not include AdvancedAuditing=false."
          reference     : "800-171|3.3.1,800-171|3.3.2,800-53|AU-12,CN-L3|7.1.3.3(a),CN-L3|7.1.3.3(b),CN-L3|7.1.3.3(c),CN-L3|8.1.3.5(a),CN-L3|8.1.3.5(b),CN-L3|8.1.4.3(a),CSCv6|14.6,CSCv7|14.9,CSF|DE.CM-1,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.PT-1,ISO/IEC-27001|A.12.4.1,ITSG-33|AU-12,LEVEL|1S,NESA|T3.6.2,NESA|T3.6.5,NESA|T3.6.6,NIAv2|SM8,SWIFT-CSCv1|6.4,TBA-FIISB|45.1.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @AUDIT_POLICY_FILE@ replaced with "/etc/kubernetes/audit-policy.yaml" in field "expect".
          expect        : "--audit-policy-file=/etc/kubernetes/audit-policy.yaml([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
# Note: Variable @AUDIT_POLICY_FILE@ replaced with "/etc/kubernetes/audit-policy.yaml" in field "description".
          description   : "1.1.37 Ensure that the AdvancedAuditing argument is not set to false - /etc/kubernetes/audit-policy.yaml"
          info          : "Do not disable advanced auditing.

Rationale:

'AdvancedAuditing' enables a much more general API auditing pipeline, which includes support for pluggable output backends and an audit policy specifying how different requests should be audited. Additionally, this enables auditing of failed authentication, authorization and login attempts which could prove crucial for protecting your production clusters. It is thus recommended not to disable advanced auditing.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
          solution      : "Follow the Kubernetes documentation and set the desired audit policy in the '/etc/kubernetes/audit-policy.yaml' file.

Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' and set the below parameters.

--audit-policy-file=/etc/kubernetes/audit-policy.yaml

In the same API server pod specification file ensure that if the '--feature-gates' argument is present, it does not include AdvancedAuditing=false."
          reference     : "800-171|3.3.1,800-171|3.3.2,800-53|AU-12,CN-L3|7.1.3.3(a),CN-L3|7.1.3.3(b),CN-L3|7.1.3.3(c),CN-L3|8.1.3.5(a),CN-L3|8.1.3.5(b),CN-L3|8.1.4.3(a),CSCv6|14.6,CSCv7|14.9,CSF|DE.CM-1,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.PT-1,ISO/IEC-27001|A.12.4.1,ITSG-33|AU-12,LEVEL|1S,NESA|T3.6.2,NESA|T3.6.5,NESA|T3.6.6,NIAv2|SM8,SWIFT-CSCv1|6.4,TBA-FIISB|45.1.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @AUDIT_POLICY_FILE@ replaced with "/etc/kubernetes/audit-policy.yaml" in field "cmd".
          cmd           : "cat /etc/kubernetes/audit-policy.yaml"
          expect        : "MANUAL_REVIEW"
          dont_echo_cmd : YES
          severity      : MEDIUM
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.37 Ensure that the AdvancedAuditing argument is not set to false - AdvancedAuditing"
          info          : "Do not disable advanced auditing.

Rationale:

'AdvancedAuditing' enables a much more general API auditing pipeline, which includes support for pluggable output backends and an audit policy specifying how different requests should be audited. Additionally, this enables auditing of failed authentication, authorization and login attempts which could prove crucial for protecting your production clusters. It is thus recommended not to disable advanced auditing."
          solution      : "Follow the Kubernetes documentation and set the desired audit policy in the '/etc/kubernetes/audit-policy.yaml' file.

Then, edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' and set the below parameters.

--audit-policy-file=/etc/kubernetes/audit-policy.yaml

In the same API server pod specification file ensure that if the '--feature-gates' argument is present, it does not include AdvancedAuditing=false."
          reference     : "800-171|3.3.1,800-171|3.3.2,800-53|AU-12,CN-L3|7.1.3.3(a),CN-L3|7.1.3.3(b),CN-L3|7.1.3.3(c),CN-L3|8.1.3.5(a),CN-L3|8.1.3.5(b),CN-L3|8.1.4.3(a),CSCv6|14.6,CSCv7|14.9,CSF|DE.CM-1,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.PT-1,ISO/IEC-27001|A.12.4.1,ITSG-33|AU-12,LEVEL|1S,NESA|T3.6.2,NESA|T3.6.5,NESA|T3.6.6,NIAv2|SM8,SWIFT-CSCv1|6.4,TBA-FIISB|45.1.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "^((?!--feature-gates=[A-z,=]*AdvancedAuditing=false).)*$"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.38 Ensure that the --request-timeout argument is set as appropriate"
          info          : "Set global request timeout for API server requests as appropriate.

Rationale:

Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' and set the below parameter as appropriate and if needed. For example,

--request-timeout=300s"
          reference     : "800-53|SC-5,CSCv6|14.6,CSCv7|14.9,CSF|DE.CM-1,CSF|PR.DS-4,ITSG-33|SC-5,LEVEL|1S,NESA|T3.3.1,NIAv2|GS10c,NIAv2|GS8e"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
# Note: Variable @REQUEST_TIMEOUT@ replaced with "300" in field "expect".
          expect        : "(--request-timeout=300([\\s]|$)|^((?!--request-timeout).)*$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.1.39 Ensure that the --authorization-mode argument includes RBAC"
          info          : "Turn on Role Based Access Control.

Rationale:

Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorisation mode."
          solution      : "Edit the API server pod specification file '/etc/kubernetes/manifests/kube-apiserver.yaml' on the master node and set the '--authorization-mode' parameter to a value that includes 'RBAC', for example:

--authorization-mode=Node,RBAC"
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|9.1,CSCv7|9.2,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-apiserver | grep -v grep"
          expect        : "--authorization-mode=[A-z,]*RBAC[A-z,]*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : PROCESS_CHECK
          description : "Check if Scheduler is running"
          name        : "kube-scheduler"
          status      : ON
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.2.1 Ensure that the --profiling argument is set to false"
          info          : "Disable profiling, if not needed.

Rationale:

Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface."
          solution      : "Edit the Scheduler pod specification file '/etc/kubernetes/manifests/kube-scheduler.yaml' file on the master node and set the below parameter.

--profiling=false"
          reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|14,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-scheduler | grep -v grep"
          expect        : "--profiling=false"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.2.2 Ensure that the --address argument is set to 127.0.0.1"
          info          : "Do not bind the scheduler service to non-loopback insecure addresses.

Rationale:

The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface"
          solution      : "Edit the Scheduler pod specification file '/etc/kubernetes/manifests/kube-scheduler.yaml' on the master node and ensure the correct value for the '--address' parameter"
          reference     : "800-171|3.4.6,800-171|3.4.7,800-53|CM-7,CN-L3|7.1.3.5(c),CN-L3|7.1.3.7(d),CN-L3|8.1.4.4(b),CSCv6|9.1,CSCv7|9.2,CSF|PR.IP-1,CSF|PR.PT-3,ITSG-33|CM-7,LEVEL|1S,NIAv2|SS13b,NIAv2|SS14a,NIAv2|SS14c,NIAv2|SS15a,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-scheduler | grep -v grep"
          expect        : "--address=127.0.0.1([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : PROCESS_CHECK
          description : "Check if Controller Manager is running"
          name        : "kube-controller"
          status      : ON
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.3.1 Ensure that the --terminated-pod-gc-threshold argument is set as appropriate"
          info          : "Activate garbage collector on pod termination, as appropriate.

Rationale:

Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection."
          solution      : "Edit the Controller Manager pod specification file '/etc/kubernetes/manifests/kube-controller-manager.yaml' on the master node and set the '--terminated-pod-gc-threshold' to an appropriate threshold, for example:

--terminated-pod-gc-threshold=10"
          reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|14,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-controller-manager | grep -v grep"
# Note: Variable @TERMINATED_POD_GC_THRESHOLD@ replaced with "12500" in field "expect".
          expect        : "--terminated-pod-gc-threshold=12500([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.3.2 Ensure that the --profiling argument is set to false"
          info          : "Disable profiling, if not needed.

Rationale:

Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface."
          solution      : "Edit the Controller Manager pod specification file '/etc/kubernetes/manifests/kube-controller-manager.yaml' on the master node and set the below parameter.

--profiling=false"
          reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|14,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-controller-manager | grep -v grep"
          expect        : "--profiling=false"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.3.3 Ensure that the --use-service-account-credentials argument is set to true"
          info          : "Use individual service account credentials for each controller.

Rationale:

The controller manager creates a service account per controller in the 'kube-system' namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the '--use-service-account-credentials' to 'true' runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks."
          solution      : "Edit the Controller Manager pod specification file '/etc/kubernetes/manifests/kube-controller-manager.yaml' on the master node to set the below parameter.

--use-service-account-credentials=true"
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-controller-manager | grep -v grep"
          expect        : "--use-service-account-credentials=true"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.3.4 Ensure that the --service-account-private-key-file  argument is set as appropriate"
          info          : "Explicitly set a service account private key file for service accounts on the controller manager.

Rationale:

To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with '--service-account-private-key-file' as appropriate."
          solution      : "Edit the Controller Manager pod specification file '/etc/kubernetes/manifests/kube-controller-manager.yaml' on the master node and set the '--service-account-private-key-file' parameter to the private key file for service accounts.

--service-account-private-key-file="
          reference     : "800-53|IA-5(2),CSCv6|14,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-controller-manager | grep -v grep"
# Note: Variable @SERVICE_ACCOUNT_PRIVATE_KEY_FILE@ replaced with "/etc/kubernetes/pki/sa.key" in field "expect".
          expect        : "--service-account-private-key-file=/etc/kubernetes/pki/sa.key([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.3.5 Ensure that the --root-ca-file argument is set as appropriate"
          info          : "Allow pods to verify the API server's serving certificate before establishing connections.

Rationale:

Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks.

Providing the root certificate for the API server's serving certificate to the controller manager with the '--root-ca-file' argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server."
          solution      : "Edit the Controller Manager pod specification file '/etc/kubernetes/manifests/kube-controller-manager.yaml' on the master node and set the '--root-ca-file' parameter to the certificate bundle file'.

--root-ca-file="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-controller-manager | grep -v grep"
# Note: Variable @ROOT_CA_FILE@ replaced with "/etc/kubernetes/pki/ca.crt" in field "expect".
          expect        : "--root-ca-file=/etc/kubernetes/pki/ca.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true"
          info          : "Enable kubelet server certificate rotation on controller-manager.

Rationale:

'RotateKubeletServerCertificate' causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad.

Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself."
          solution      : "Edit the Controller Manager pod specification file '/etc/kubernetes/manifests/kube-controller-manager.yaml' on the master node and set the '--feature-gates' parameter to include 'RotateKubeletServerCertificate=true'.

--feature-gates=RotateKubeletServerCertificate=true"
          reference     : "800-53|CM-3(6),CSCv6|14.2,CSCv7|14.4,CSF|PR.IP-1,CSF|PR.IP-3,ISO/IEC-27001|A.10.1.1,LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-controller-manager | grep -v grep"
          expect        : "--feature-gates=[A-z,=]*RotateKubeletServerCertificate=true[A-z,=]*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.3.7 Ensure that the --address argument is set to 127.0.0.1"
          info          : "Do not bind the Controller Manager service to non-loopback insecure addresses.

Rationale:

The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface"
          solution      : "Edit the Controller Manager pod specification file '/etc/kubernetes/manifests/kube-controller-manager.yaml' on the master node and ensure the correct value for the '--address' parameter"
          reference     : "800-171|3.4.6,800-171|3.4.7,800-53|CM-7,CN-L3|7.1.3.5(c),CN-L3|7.1.3.7(d),CN-L3|8.1.4.4(b),CSCv6|9.1,CSCv7|9.2,CSF|PR.IP-1,CSF|PR.PT-3,ITSG-33|CM-7,LEVEL|1S,NIAv2|SS13b,NIAv2|SS14a,NIAv2|SS14c,NIAv2|SS15a,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kube-controller-manager | grep -v grep"
          expect        : "--address=127.0.0.1([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : PROCESS_CHECK
          description : "Check if API Server is running"
          name        : "kube-apiserver"
          status      : ON
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.1 Ensure that the API server pod specification file permissions are set to 644 or more restrictive"
          info        : "Ensure that the API server pod specification file has permissions of '644' or more restrictive.

Rationale:

The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod 644 /etc/kubernetes/manifests/kube-apiserver.yaml"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @APISERVER_FILE@ replaced with "/etc/kubernetes/manifests/kube-apiserver.yaml" in field "file".
          file        : "/etc/kubernetes/manifests/kube-apiserver.yaml"
          mask        : "133"
        </custom_item>

        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.2 Ensure that the API server pod specification file ownership is set to root:root"
          info        : "Ensure that the API server pod specification file ownership is set to 'root:root'.

Rationale:

The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @APISERVER_FILE@ replaced with "/etc/kubernetes/manifests/kube-apiserver.yaml" in field "file".
          file        : "/etc/kubernetes/manifests/kube-apiserver.yaml"
          owner       : "root"
          group       : "root"
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : PROCESS_CHECK
          description : "Check if Controller Manager is running"
          name        : "kube-controller"
          status      : ON
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.3 Ensure that the controller manager pod specification file permissions are set to 644 or more restrictive"
          info        : "Ensure that the controller manager pod specification file has permissions of '644' or more restrictive.

Rationale:

The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod 644 /etc/kubernetes/manifests/kube-controller-manager.yaml"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @CONTROLLER_MANAGER_CONFIG_FILE@ replaced with "/etc/kubernetes/manifests/kube-controller-manager.yaml" in field "file".
          file        : "/etc/kubernetes/manifests/kube-controller-manager.yaml"
          mask        : "133"
        </custom_item>

        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.4 Ensure that the controller manager pod specification file ownership is set to root:root"
          info        : "Ensure that the controller manager pod specification file ownership is set to 'root:root'.

Rationale:

The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @CONTROLLER_MANAGER_CONFIG_FILE@ replaced with "/etc/kubernetes/manifests/kube-controller-manager.yaml" in field "file".
          file        : "/etc/kubernetes/manifests/kube-controller-manager.yaml"
          owner       : "root"
          group       : "root"
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : PROCESS_CHECK
          description : "Check if Scheduler is running"
          name        : "kube-scheduler"
          status      : ON
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.5 Ensure that the scheduler pod specification file permissions are set to 644 or more restrictive"
          info        : "Ensure that the scheduler pod specification file has permissions of '644' or more restrictive.

Rationale:

The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod 644 /etc/kubernetes/manifests/kube-scheduler.yaml"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @SCHEDULER_FILE@ replaced with "/etc/kubernetes/manifests/kube-scheduler.yaml" in field "file".
          file        : "/etc/kubernetes/manifests/kube-scheduler.yaml"
          mask        : "133"
        </custom_item>

        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.6 Ensure that the scheduler pod specification file ownership is set to root:root"
          info        : "Ensure that the scheduler pod specification file ownership is set to 'root:root'.

Rationale:

The scheduler pod specification file controls various parameters that set the behavior of the 'kube-scheduler' service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @SCHEDULER_FILE@ replaced with "/etc/kubernetes/manifests/kube-scheduler.yaml" in field "file".
          file        : "/etc/kubernetes/manifests/kube-scheduler.yaml"
          owner       : "root"
          group       : "root"
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "Check if etcd is running"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
          expect        : "etcd[2]?[\\s]"
          dont_echo_cmd : YES
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.7 Ensure that the etcd pod specification file permissions are set to 644 or more restrictive"
          info        : "Ensure that the '/etc/kubernetes/manifests/etcd.yaml' file has permissions of '644' or more restrictive.

Rationale:

The etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' controls various parameters that set the behavior of the 'etcd' service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod 644 /etc/kubernetes/manifests/etcd.yaml"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @ETCD_CONF_FILE@ replaced with "/etc/kubernetes/manifests/etcd.yaml" in field "file".
          file        : "/etc/kubernetes/manifests/etcd.yaml"
          mask        : "133"
        </custom_item>

        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.8 Ensure that the etcd pod specification file ownership is set to root:root"
          info        : "Ensure that the '/etc/kubernetes/manifests/etcd.yaml' file ownership is set to 'root:root'.

Rationale:

The etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' controls various parameters that set the behavior of the 'etcd' service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown root:root /etc/kubernetes/manifests/etcd.yaml"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @ETCD_CONF_FILE@ replaced with "/etc/kubernetes/manifests/etcd.yaml" in field "file".
          file        : "/etc/kubernetes/manifests/etcd.yaml"
          owner       : "root"
          group       : "root"
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          type        : FILE_CHECK
          description : "Check if flanneld file exists"
# Note: Variable @FLANNELD_FILE@ replaced with "/etc/cni/net.d/10-flannel.conf" in field "file".
          file        : "/etc/cni/net.d/10-flannel.conf"
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.9 Ensure that the Container Network Interface file permissions are set to 644 or more restrictive"
          info        : "Ensure that the Container Network Interface files have permissions of '644' or more restrictive.

Rationale:

Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod 644"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @FLANNELD_FILE@ replaced with "/etc/cni/net.d/10-flannel.conf" in field "file".
          file        : "/etc/cni/net.d/10-flannel.conf"
          mask        : "133"
        </custom_item>

        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.10 Ensure that the Container Network Interface file ownership is set to root:root"
          info        : "Ensure that the Container Network Interface files have ownership set to 'root:root'.

Rationale:

Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by 'root:root'."
          solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown root:root"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @FLANNELD_FILE@ replaced with "/etc/cni/net.d/10-flannel.conf" in field "file".
          file        : "/etc/cni/net.d/10-flannel.conf"
          owner       : "root"
          group       : "root"
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "Check if etcd is running"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
          expect        : "etcd[2]?[\\s]"
          dont_echo_cmd : YES
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.11 Ensure that the etcd data directory permissions are set to 700 or more restrictive"
          info        : "Ensure that the etcd data directory has permissions of '700' or more restrictive.

Rationale:

etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world."
          solution    : "On the etcd server node, get the etcd data directory, passed as an argument '--data-dir', from the below command:

ps -ef | grep etcd

Run the below command (based on the etcd data directory found above). For example,

chmod 700 /var/lib/etcd"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|14,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @ETCD_DATA_DIR@ replaced with "/var/lib/etcd" in field "file".
          file        : "/var/lib/etcd"
          mask        : "077"
        </custom_item>

        <custom_item>
          system      : "Linux"
          type        : FILE_CHECK
          description : "1.4.12 Ensure that the etcd data directory ownership is set to etcd:etcd"
          info        : "Ensure that the etcd data directory ownership is set to 'etcd:etcd'.

Rationale:

etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by 'etcd:etcd'."
          solution    : "On the etcd server node, get the etcd data directory, passed as an argument '--data-dir', from the below command:

ps -ef | grep etcd

Run the below command (based on the etcd data directory found above). For example,

chown etcd:etcd /var/lib/etcd"
          reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|14,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
          see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @ETCD_DATA_DIR@ replaced with "/var/lib/etcd" in field "file".
          file        : "/var/lib/etcd"
          owner       : "etcd"
          group       : "etcd"
        </custom_item>
      </then>
    </if>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.13 Ensure that the admin.conf file permissions are set to 644 or more restrictive"
      info        : "Ensure that the 'admin.conf' file has permissions of '644' or more restrictive.

Rationale:

The 'admin.conf' is the administrator kubeconfig file defining various settings for the administration of the cluster. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod 644 /etc/kubernetes/admin.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @ADMIN_KUBECONFIG_FILE@ replaced with "/etc/kubernetes/admin.conf" in field "file".
      file        : "/etc/kubernetes/admin.conf"
      mask        : "133"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.14 Ensure that the admin.conf file ownership is set to root:root"
      info        : "Ensure that the 'admin.conf' file ownership is set to 'root:root'.

Rationale:

The 'admin.conf' file contains the admin credentials for the cluster. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown root:root /etc/kubernetes/admin.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @ADMIN_KUBECONFIG_FILE@ replaced with "/etc/kubernetes/admin.conf" in field "file".
      file        : "/etc/kubernetes/admin.conf"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.15 Ensure that the scheduler.conf file permissions are set to 644 or more restrictive"
      info        : "Ensure that the 'scheduler.conf' file has permissions of '644' or more restrictive.

Rationale:

The 'scheduler.conf' file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod 644 /etc/kubernetes/scheduler.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @SCHEDULER_KUBECONFIG_FILE@ replaced with "/etc/kubernetes/scheduler.conf" in field "file".
      file        : "/etc/kubernetes/scheduler.conf"
      mask        : "133"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.16 Ensure that the scheduler.conf file ownership is set to root:root"
      info        : "Ensure that the 'scheduler.conf' file ownership is set to 'root:root'.

Rationale:

The 'scheduler.conf' file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown root:root /etc/kubernetes/scheduler.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @SCHEDULER_KUBECONFIG_FILE@ replaced with "/etc/kubernetes/scheduler.conf" in field "file".
      file        : "/etc/kubernetes/scheduler.conf"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.17 Ensure that the controller-manager.conf file permissions are set to 644 or more restrictive"
      info        : "Ensure that the 'controller-manager.conf' file has permissions of 644 or more restrictive.

Rationale:

The 'controller-manager.conf' file is the kubeconfig file for the Controller Manager. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod 644 /etc/kubernetes/controller-manager.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @CONTROLLER_MANAGER_KUBECONFIG_FILE@ replaced with "/etc/kubernetes/controller-manager.conf" in field "file".
      file        : "/etc/kubernetes/controller-manager.conf"
      mask        : "133"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.18 Ensure that the controller-manager.conf file ownership is set to root:root"
      info        : "Ensure that the 'controller-manager.conf' file ownership is set to 'root:root'.

Rationale:

The 'controller-manager.conf' file is the kubeconfig file for the Controller Manager. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown root:root /etc/kubernetes/controller-manager.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @CONTROLLER_MANAGER_KUBECONFIG_FILE@ replaced with "/etc/kubernetes/controller-manager.conf" in field "file".
      file        : "/etc/kubernetes/controller-manager.conf"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.19 Ensure that the Kubernetes PKI directory and file ownership is set to root:root"
      info        : "Ensure that the Kubernetes PKI directory and file ownership is set to 'root:root'.

Rationale:

Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by 'root:root'."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chown -R root:root /etc/kubernetes/pki/"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @PKI_DIRECTORY@ replaced with "/etc/kubernetes/pki" in field "file".
      file        : "/etc/kubernetes/pki"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.20 Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive"
      info        : "Ensure that Kubernetes PKI certificate files have permissions of '644' or more restrictive.

Rationale:

Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to '644' or more restrictive to protect their integrity."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod -R 644 /etc/kubernetes/pki/*.crt"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @PKI_DIRECTORY@ replaced with "/etc/kubernetes/pki" in field "file".
      file        : "/etc/kubernetes/pki/*.crt"
      mask        : "133"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "1.4.21 Ensure that the Kubernetes PKI key file permissions are set to 600"
      info        : "Ensure that Kubernetes PKI key files have permissions of '600'.

Rationale:

Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to '600' to protect their integrity and confidentiality."
      solution    : "Run the below command (based on the file location on your system) on the master node. For example,

chmod -R 600 /etc/kubernetes/pki/*.key"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @PKI_DIRECTORY@ replaced with "/etc/kubernetes/pki" in field "file".
      file        : "/etc/kubernetes/pki/*.key"
      mask        : "177"
    </custom_item>

    <if>
      <condition type:"AND">
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "Check if etcd is running"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
          expect        : "etcd[2]?[\\s]"
          dont_echo_cmd : YES
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.5.1 Ensure that the --cert-file and --key-file arguments are set as appropriate - cert"
          info          : "Configure TLS encryption for the etcd service.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit."
          solution      : "Follow the etcd service documentation and configure TLS encryption.

Then, edit the etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' on the master node and set the below parameters.

--cert-file=
--key-file="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
# Note: Variable @CERT_FILE@ replaced with "/etc/kubernetes/pki/etcd.crt" in field "expect".
          expect        : "--cert-file=/etc/kubernetes/pki/etcd.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.5.1 Ensure that the --cert-file and --key-file arguments are set as appropriate - key"
          info          : "Configure TLS encryption for the etcd service.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit."
          solution      : "Follow the etcd service documentation and configure TLS encryption.

Then, edit the etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' on the master node and set the below parameters.

--cert-file=
--key-file="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
# Note: Variable @KEY_FILE@ replaced with "/etc/kubernetes/pki/etcd.key" in field "expect".
          expect        : "--key-file=/etc/kubernetes/pki/etcd.key([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.5.2 Ensure that the --client-cert-auth argument is set to true"
          info          : "Enable client authentication on etcd service.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service."
          solution      : "Edit the etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' on the master node and set the below parameter.

--client-cert-auth='true'"
          reference     : "800-53|IA-5(2),CSCv6|14,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
          expect        : "--client-cert-auth=true"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.5.3 Ensure that the --auto-tls argument is not set to true"
          info          : "Do not use self-signed certificates for TLS.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service."
          solution      : "Edit the etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' on the master node and either remove the '--auto-tls' parameter or set it to 'false'.

--auto-tls=false"
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
          expect        : "^((?!--auto-tls=true).)*$"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.5.4 Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate - cert"
          info          : "etcd should be configured to make use of TLS encryption for peer connections.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters."
          solution      : "Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster.

Then, edit the etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' on the master node and set the below parameters.

--peer-client-file=
--peer-key-file="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
# Note: Variable @PEER_CERT_FILE@ replaced with "/etc/kubernetes/pki/etcd/peer.crt" in field "expect".
          expect        : "--peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.5.4 Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate - key"
          info          : "etcd should be configured to make use of TLS encryption for peer connections.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters."
          solution      : "Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster.

Then, edit the etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' on the master node and set the below parameters.

--peer-client-file=
--peer-key-file="
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
# Note: Variable @PEER_KEY_FILE@ replaced with "/etc/kubernetes/pki/etcd/peer.key" in field "expect".
          expect        : "--peer-key-file=/etc/kubernetes/pki/etcd/peer.key([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.5.5 Ensure that the --peer-client-cert-auth argument is set to true"
          info          : "etcd should be configured for peer authentication.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
          solution      : "Edit the etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' on the master node and set the below parameter.

--peer-client-cert-auth=true"
          reference     : "800-53|IA-5(2),CSCv6|14.4,CSCv7|14.6,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
          expect        : "--peer-client-cert-auth=true"
          dont_echo_cmd : YES
          severity      : MEDIUM
        </custom_item>

        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "1.5.6 Ensure that the --peer-auto-tls argument is not set to true"
          info          : "Do not use automatically generated self-signed certificates for TLS connections between peers.

Rationale:

etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self-signed certificates for authentication.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
          solution      : "Edit the etcd pod specification file '/etc/kubernetes/manifests/etcd.yaml' on the master node and either remove the '--peer-auto-tls' parameter or set it to 'false'.

--peer-auto-tls=false"
          reference     : "800-53|IA-5(2),CSCv6|14,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | egrep '/etcd[2]?([[:space:]]|$)' | grep -v grep"
          expect        : "^((?!--peer-auto-tls=true).)*$"
          dont_echo_cmd : YES
          severity      : MEDIUM
        </custom_item>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          type        : FILE_CHECK
          description : "Check if kubectl exists"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "file".
          file        : "/usr/bin/kubectl"
        </custom_item>
      </condition>

      <then>
        <if>
          <condition type:"AND">
            <custom_item>
              type        : FILE_CHECK
              description : "Check if kubeconfig exists"
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "file".
              file        : "/etc/kubernetes/admin.conf"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.6.1 Ensure that the cluster-admin role is only used where required"
              info          : "The RBAC role 'cluster-admin' provides wide-ranging powers over the environment and should be used only where and when needed.

Rationale:

Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as 'cluster-admin' provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as 'cluster-admin' allow super-user access to perform any action on any resource. When used in a 'ClusterRoleBinding', it gives full control over every resource in the cluster and in all namespaces. When used in a 'RoleBinding', it gives full control over every resource in the rolebinding's namespace, including the namespace itself.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
              solution      : "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges.

Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role :

kubectl delete clusterrolebinding [name]"
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "cmd".
              cmd           : "/usr/bin/kubectl get clusterrolebindings --kubeconfig /etc/kubernetes/admin.conf -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name"
              expect        : "MANUAL_REVIEW"
              dont_echo_cmd : YES
              severity      : MEDIUM
            </custom_item>
          </then>

          <else>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.6.1 Ensure that the cluster-admin role is only used where required"
              info          : "The RBAC role 'cluster-admin' provides wide-ranging powers over the environment and should be used only where and when needed.

Rationale:

Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as 'cluster-admin' provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as 'cluster-admin' allow super-user access to perform any action on any resource. When used in a 'ClusterRoleBinding', it gives full control over every resource in the cluster and in all namespaces. When used in a 'RoleBinding', it gives full control over every resource in the rolebinding's namespace, including the namespace itself.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
              solution      : "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges.

Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role :

kubectl delete clusterrolebinding [name]"
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
              cmd           : "/usr/bin/kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name"
              expect        : "MANUAL_REVIEW"
              dont_echo_cmd : YES
              severity      : MEDIUM
            </custom_item>
          </else>
        </if>

        <if>
          <condition type:"AND">
            <custom_item>
              type        : FILE_CHECK
              description : "Check if kubeconfig exists"
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "file".
              file        : "/etc/kubernetes/admin.conf"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.6.2 Create administrative boundaries between resources using namespaces"
              info          : "Use namespaces to isolate your Kubernetes objects.

Rationale:

Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called 'default'. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
              solution      : "Follow the documentation and create namespaces for objects in your deployment as you need them."
              reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "cmd".
              cmd           : "/usr/bin/kubectl get namespaces --kubeconfig /etc/kubernetes/admin.conf"
              expect        : "MANUAL_REVIEW"
              dont_echo_cmd : YES
              severity      : MEDIUM
            </custom_item>
          </then>

          <else>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.6.2 Create administrative boundaries between resources using namespaces"
              info          : "Use namespaces to isolate your Kubernetes objects.

Rationale:

Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called 'default'. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
              solution      : "Follow the documentation and create namespaces for objects in your deployment as you need them."
              reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
              cmd           : "/usr/bin/kubectl get namespaces"
              expect        : "MANUAL_REVIEW"
              dont_echo_cmd : YES
              severity      : MEDIUM
            </custom_item>
          </else>
        </if>

        <if>
          <condition type:"AND">
            <custom_item>
              type        : FILE_CHECK
              description : "Check if kubeconfig exists"
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "file".
              file        : "/etc/kubernetes/admin.conf"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.1 Do not admit privileged containers"
              info          : "Do not generally permit containers to be run with the 'securityContext.privileged' flag set to 'true'.

Rationale:

Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit privileged containers.

If you need to run privileged containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.privileged' field is omitted or set to 'false'."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp --kubeconfig /etc/kubernetes/admin.conf -o=custom-columns=name:.metadata.name,privileged:.spec.privileged"
              expect        : "^.*[\\s]<none>$"
              dont_echo_cmd : YES
            </custom_item>
          </then>

          <else>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.1 Do not admit privileged containers"
              info          : "Do not generally permit containers to be run with the 'securityContext.privileged' flag set to 'true'.

Rationale:

Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit privileged containers.

If you need to run privileged containers, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.privileged' field is omitted or set to 'false'."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp -o=custom-columns=name:.metadata.name,privileged:.spec.privileged"
              expect        : "^.*[\\s]<none>$"
              dont_echo_cmd : YES
            </custom_item>
          </else>
        </if>

        <if>
          <condition type:"AND">
            <custom_item>
              type        : FILE_CHECK
              description : "Check if kubeconfig exists"
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "file".
              file        : "/etc/kubernetes/admin.conf"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.2 Do not admit containers wishing to share the host process ID namespace"
              info          : "Do not generally permit containers to be run with the 'hostPID' flag set to true.

Rationale:

A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host PID namespace.

If you need to run containers which require hostPID, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.hostPID' field is omitted or set to false."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp --kubeconfig /etc/kubernetes/admin.conf -o=custom-columns=name:.metadata.name,hostPID:.spec.hostPID"
              expect        : "^.*[\\s]<none>$"
              dont_echo_cmd : YES
            </custom_item>
          </then>

          <else>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.2 Do not admit containers wishing to share the host process ID namespace"
              info          : "Do not generally permit containers to be run with the 'hostPID' flag set to true.

Rationale:

A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host PID namespace.

If you need to run containers which require hostPID, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.hostPID' field is omitted or set to false."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp -o=custom-columns=name:.metadata.name,hostPID:.spec.hostPID"
              expect        : "^.*[\\s]<none>$"
              dont_echo_cmd : YES
            </custom_item>
          </else>
        </if>

        <if>
          <condition type:"AND">
            <custom_item>
              type        : FILE_CHECK
              description : "Check if kubeconfig exists"
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "file".
              file        : "/etc/kubernetes/admin.conf"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.3 Do not admit containers wishing to share the host IPC namespace"
              info          : "Do not generally permit containers to be run with the 'hostIPC' flag set to true.

Rationale:

A container running in the host's IPC namespace can use IPC to interact with processes outside the container.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host IPC namespace.

If you have a requirement to containers which require hostIPC, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.hostIPC' field is omitted or set to false."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp --kubeconfig /etc/kubernetes/admin.conf -o=custom-columns=name:.metadata.name,hostIPC:.spec.hostIPC"
              expect        : "^.*[\\s]<none>$"
              dont_echo_cmd : YES
            </custom_item>
          </then>

          <else>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.3 Do not admit containers wishing to share the host IPC namespace"
              info          : "Do not generally permit containers to be run with the 'hostIPC' flag set to true.

Rationale:

A container running in the host's IPC namespace can use IPC to interact with processes outside the container.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host IPC namespace.

If you have a requirement to containers which require hostIPC, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.hostIPC' field is omitted or set to false."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp -o=custom-columns=name:.metadata.name,hostIPC:.spec.hostIPC"
              expect        : "^.*[\\s]<none>$"
              dont_echo_cmd : YES
            </custom_item>
          </else>
        </if>

        <if>
          <condition type:"AND">
            <custom_item>
              type        : FILE_CHECK
              description : "Check if kubeconfig exists"
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "file".
              file        : "/etc/kubernetes/admin.conf"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.4 Do not admit containers wishing to share the host network namespace"
              info          : "Do not generally permit containers to be run with the 'hostNetwork' flag set to true.

Rationale:

A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host network namespace.

If you have need to run containers which require hostNetwork, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.hostNetwork' field is omitted or set to false."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp --kubeconfig /etc/kubernetes/admin.conf -o=custom-columns=name:.metadata.name,hostNetwork:.spec.hostNetwork"
              expect        : "^.*[\\s]<none>$"
              dont_echo_cmd : YES
            </custom_item>
          </then>

          <else>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.4 Do not admit containers wishing to share the host network namespace"
              info          : "Do not generally permit containers to be run with the 'hostNetwork' flag set to true.

Rationale:

A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to share the host network namespace.

If you have need to run containers which require hostNetwork, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.hostNetwork' field is omitted or set to false."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp -o=custom-columns=name:.metadata.name,hostNetwork:.spec.hostNetwork"
              expect        : "^.*[\\s]<none>$"
              dont_echo_cmd : YES
            </custom_item>
          </else>
        </if>

        <if>
          <condition type:"AND">
            <custom_item>
              type        : FILE_CHECK
              description : "Check if kubeconfig exists"
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "file".
              file        : "/etc/kubernetes/admin.conf"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.5 Do not admit containers with allowPrivilegeEscalation"
              info          : "Do not generally permit containers to be run with the 'allowPrivilegeEscalation' flag set to true.

Rationale:

A container running with the 'allowPrivilegeEscalation' flag set to 'true' may have processes that can gain more privileges than their parent.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run.

If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.allowPrivilegeEscalation' field is omitted or set to false."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
# Note: Variable @KUBECONFIG@ replaced with "/etc/kubernetes/admin.conf" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp --kubeconfig /etc/kubernetes/admin.conf -o=custom-columns=name:.metadata.name,allowPrivilegeEscalation:.spec.allowPrivilegeEscalation"
              expect        : "^.*[\\s]false$"
              dont_echo_cmd : YES
            </custom_item>
          </then>

          <else>
            <custom_item>
              system        : "Linux"
              type          : CMD_EXEC
              description   : "1.7.5 Do not admit containers with allowPrivilegeEscalation"
              info          : "Do not generally permit containers to be run with the 'allowPrivilegeEscalation' flag set to true.

Rationale:

A container running with the 'allowPrivilegeEscalation' flag set to 'true' may have processes that can gain more privileges than their parent.

There should be at least one PodSecurityPolicy (PSP) defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run.

If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate PSP and you should carefully check RBAC controls to ensure that only limited service accounts and users are given permission to access that PSP."
              solution      : "Create a PSP as described in the Kubernetes documentation, ensuring that the '.spec.allowPrivilegeEscalation' field is omitted or set to false."
              reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also      : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBECTL_PATH@ replaced with "/usr/bin" in field "cmd".
              cmd           : "/usr/bin/kubectl get psp -o=custom-columns=name:.metadata.name,allowPrivilegeEscalation:.spec.allowPrivilegeEscalation"
              expect        : "^.*[\\s]false$"
              dont_echo_cmd : YES
            </custom_item>
          </else>
        </if>
      </then>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.1 Ensure that the --anonymous-auth argument is set to false"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--anonymous-auth="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.1 Ensure that the --anonymous-auth argument is set to false"
          info          : "Disable anonymous requests to the Kubelet server.

Rationale:

When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests."
          solution      : "If using a Kubelet config file, edit the file to set 'authentication: anonymous: enabled' to 'false'.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--anonymous-auth=false

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-171|3.5.1,800-53|IA-2,CN-L3|7.1.3.1(a),CN-L3|7.1.3.1(e),CN-L3|8.1.4.1(a),CN-L3|8.1.4.2(a),CN-L3|8.5.4.1(a),CSCv6|14,CSF|PR.AC-1,ITSG-33|IA-2,LEVEL|1S,NESA|T2.3.8,NESA|T5.3.1,NESA|T5.4.2,NESA|T5.5.1,NESA|T5.5.2,NESA|T5.5.3,NIAv2|AM14b,NIAv2|AM2,NIAv2|AM8,TBA-FIISB|35.1,TBA-FIISB|36.1"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--anonymous-auth=false"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "2.1.1 Ensure that the --anonymous-auth argument is set to false"
              info        : "Disable anonymous requests to the Kubelet server.

Rationale:

When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests."
              solution    : "If using a Kubelet config file, edit the file to set 'authentication: anonymous: enabled' to 'false'.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--anonymous-auth=false

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.5.1,800-53|IA-2,CN-L3|7.1.3.1(a),CN-L3|7.1.3.1(e),CN-L3|8.1.4.1(a),CN-L3|8.1.4.2(a),CN-L3|8.5.4.1(a),CSCv6|14,CSF|PR.AC-1,ITSG-33|IA-2,LEVEL|1S,NESA|T2.3.8,NESA|T5.3.1,NESA|T5.4.2,NESA|T5.5.1,NESA|T5.5.2,NESA|T5.5.3,NIAv2|AM14b,NIAv2|AM2,NIAv2|AM8,TBA-FIISB|35.1,TBA-FIISB|36.1"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "cmd".
              cmd         : "sed 's/\\(\\s*\\)\\(.*:\\)\\s*\\(.*\\)/\\1\\|\\2\\|\\3/g' '/var/lib/kubelet/config.yaml' | awk -F'|' '{ indent = length($1)/2; vname[indent] = $2; for (i in vname) {if (i > indent) {delete vname[i]}} if (length($3) > 0) { vn=none; for (i=0; i<=indent; i++) {vn=(vn)(vname[i])} { print vn $3 } } }'"
              expect      : "^authentication:anonymous:enabled:false$"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.1 Ensure that the --anonymous-auth argument is set to false"
              info        : "Disable anonymous requests to the Kubelet server.

Rationale:

When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests."
              solution    : "If using a Kubelet config file, edit the file to set 'authentication: anonymous: enabled' to 'false'.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--anonymous-auth=false

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.5.1,800-53|IA-2,CN-L3|7.1.3.1(a),CN-L3|7.1.3.1(e),CSCv6|14,CSF|PR.AC-1,ITSG-33|IA-2,LEVEL|1S,NESA|T2.3.8,NESA|T5.3.1,NESA|T5.4.2,NESA|T5.5.1,NESA|T5.5.2,NESA|T5.5.3,NIAv2|AM14b,NIAv2|AM2,NIAv2|AM8"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--authorization-mode="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow"
          info          : "Do not allow all requests. Enable explicit authorization.

Rationale:

Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests."
          solution      : "If using a Kubelet config file, edit the file to set 'authorization: mode' to 'Webhook'.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_AUTHZ_ARGS' variable.

--authorization-mode=Webhook

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--authorization-mode=((?!AlwaysAllow).)*([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK_NOT
              description : "2.1.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow"
              info        : "Do not allow all requests. Enable explicit authorization.

Rationale:

Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests."
              solution    : "If using a Kubelet config file, edit the file to set 'authorization: mode' to 'Webhook'.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_AUTHZ_ARGS' variable.

--authorization-mode=Webhook

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.10.6(a),CN-L3|8.1.4.2(d),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*mode[\\s]*:"
              expect      : "^[\\s]*mode[\\s]*:.*AlwaysAllow"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow"
              info        : "Do not allow all requests. Enable explicit authorization.

Rationale:

Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests."
              solution    : "If using a Kubelet config file, edit the file to set 'authorization: mode' to 'Webhook'.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_AUTHZ_ARGS' variable.

--authorization-mode=Webhook

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.1.5,800-53|AC-6,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CSCv6|14,CSF|PR.AC-4,CSF|PR.DS-5,ITSG-33|AC-6,LEVEL|1S,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,SWIFT-CSCv1|5.1"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.3 Ensure that the --client-ca-file argument is set as appropriate"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--client-ca-file="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.3 Ensure that the --client-ca-file argument is set as appropriate"
          info          : "Enable Kubelet authentication using certificates.

Rationale:

The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelets port-forwarding functionality. These connections terminate at the kubelets HTTPS endpoint. By default, the apiserver does not verify the kubelets serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests."
          solution      : "If using a Kubelet config file, edit the file to set 'authentication: x509: clientCAFile' to the location of the client CA file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_AUTHZ_ARGS' variable.

--client-ca-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @CLIENT_CA_FILE@ replaced with "/etc/kubernetes/pki/ca.crt" in field "expect".
          expect        : "--client-ca-file=/etc/kubernetes/pki/ca.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.3 Ensure that the --client-ca-file argument is set as appropriate"
              info        : "Enable Kubelet authentication using certificates.

Rationale:

The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelets port-forwarding functionality. These connections terminate at the kubelets HTTPS endpoint. By default, the apiserver does not verify the kubelets serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests."
              solution    : "If using a Kubelet config file, edit the file to set 'authentication: x509: clientCAFile' to the location of the client CA file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_AUTHZ_ARGS' variable.

--client-ca-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*clientCAFile[\\s]*:"
# Note: Variable @CLIENT_CA_FILE@ replaced with "/etc/kubernetes/pki/ca.crt" in field "expect".
              expect      : "^[\\s]*clientCAFile[\\s]*:[\\s]*/etc/kubernetes/pki/ca.crt"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.3 Ensure that the --client-ca-file argument is set as appropriate"
              info        : "Enable Kubelet authentication using certificates.

Rationale:

The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelets port-forwarding functionality. These connections terminate at the kubelets HTTPS endpoint. By default, the apiserver does not verify the kubelets serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests."
              solution    : "If using a Kubelet config file, edit the file to set 'authentication: x509: clientCAFile' to the location of the client CA file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_AUTHZ_ARGS' variable.

--client-ca-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|IA-5,CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5,LEVEL|1S"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.4 Ensure that the --read-only-port argument is set to 0"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--read-only-port="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.4 Ensure that the --read-only-port argument is set to 0"
          info          : "Disable the read-only port.

Rationale:

The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster."
          solution      : "If using a Kubelet config file, edit the file to set 'readOnlyPort' to '0'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--read-only-port=0

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-171|3.4.6,800-171|3.4.7,800-53|CM-7,CN-L3|7.1.3.5(c),CN-L3|7.1.3.7(d),CN-L3|8.1.4.4(b),CSCv6|9.1,CSCv7|9.2,CSF|PR.IP-1,CSF|PR.PT-3,ITSG-33|CM-7,LEVEL|1S,NIAv2|SS13b,NIAv2|SS14a,NIAv2|SS14c,NIAv2|SS15a,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--read-only-port=0([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.4 Ensure that the --read-only-port argument is set to 0"
              info        : "Disable the read-only port.

Rationale:

The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster."
              solution    : "If using a Kubelet config file, edit the file to set 'readOnlyPort' to '0'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--read-only-port=0

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.4.6,800-171|3.4.7,800-53|CM-7,CN-L3|7.1.3.5(c),CN-L3|7.1.3.7(d),CN-L3|8.1.4.4(b),CSCv6|9.1,CSCv7|9.2,CSF|PR.IP-1,CSF|PR.PT-3,ITSG-33|CM-7,LEVEL|1S,NIAv2|SS13b,NIAv2|SS14a,NIAv2|SS14c,NIAv2|SS15a,SWIFT-CSCv1|2.3"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*readOnlyPort[\\s]*:"
              expect      : "^[\\s]*readOnlyPort[\\s]*:[\\s]*0"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.4 Ensure that the --read-only-port argument is set to 0"
              info        : "Disable the read-only port.

Rationale:

The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster."
              solution    : "If using a Kubelet config file, edit the file to set 'readOnlyPort' to '0'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--read-only-port=0

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.4.6,800-171|3.4.7,800-53|CM-7,CN-L3|7.1.3.5(c),CN-L3|7.1.3.7(d),CSCv6|9.1,CSCv7|9.2,CSF|PR.IP-1,CSF|PR.PT-3,ITSG-33|CM-7,LEVEL|1S,NIAv2|SS13b,NIAv2|SS14a,NIAv2|SS14c,NIAv2|SS15a,SWIFT-CSCv1|2.3"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--streaming-connection-idle-timeout="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0"
          info          : "Do not disable timeouts on streaming connections.

Rationale:

Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports.

**Note:** By default, '--streaming-connection-idle-timeout' is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases."
          solution      : "If using a Kubelet config file, edit the file to set 'streamingConnectionIdleTimeout' to a value other than 0.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--streaming-connection-idle-timeout=5m

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-53|SC-5,CSCv6|9,CSF|DE.CM-1,CSF|PR.DS-4,ITSG-33|SC-5,LEVEL|1S,NESA|T3.3.1,NIAv2|GS10c,NIAv2|GS8e"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--streaming-connection-idle-timeout=[^0]"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK_NOT
              description : "2.1.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0"
              info        : "Do not disable timeouts on streaming connections.

Rationale:

Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports.

**Note:** By default, '--streaming-connection-idle-timeout' is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases."
              solution    : "If using a Kubelet config file, edit the file to set 'streamingConnectionIdleTimeout' to a value other than 0.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--streaming-connection-idle-timeout=5m

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|SC-5,CSCv6|9,CSF|DE.CM-1,CSF|PR.DS-4,ITSG-33|SC-5,LEVEL|1S,NESA|T3.3.1,NIAv2|GS10c,NIAv2|GS8e"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*streamingConnectionIdleTimeout[\\s]*:"
              expect      : "^[\\s]*streamingConnectionIdleTimeout[\\s]*:[\\s]*0"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0"
              info        : "Do not disable timeouts on streaming connections.

Rationale:

Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports.

**Note:** By default, '--streaming-connection-idle-timeout' is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases."
              solution    : "If using a Kubelet config file, edit the file to set 'streamingConnectionIdleTimeout' to a value other than 0.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--streaming-connection-idle-timeout=5m

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|SC-5,CSCv6|9,CSF|DE.CM-1,CSF|PR.DS-4,ITSG-33|SC-5,LEVEL|1S,NESA|T3.3.1,NIAv2|GS10c,NIAv2|GS8e"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.6 Ensure that the --protect-kernel-defaults argument is set to true"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--protect-kernel-defaults="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.6 Ensure that the --protect-kernel-defaults argument is set to true"
          info          : "Protect tuned kernel parameters from overriding kubelet default kernel parameter values.

Rationale:

Kernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior."
          solution      : "If using a Kubelet config file, edit the file to set 'protectKernelDefaults: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--protect-kernel-defaults=true

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|3,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--protect-kernel-defaults=true"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.6 Ensure that the --protect-kernel-defaults argument is set to true"
              info        : "Protect tuned kernel parameters from overriding kubelet default kernel parameter values.

Rationale:

Kernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior."
              solution    : "If using a Kubelet config file, edit the file to set 'protectKernelDefaults: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--protect-kernel-defaults=true

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|3,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*protectKernelDefaults[\\s]*:"
              expect      : "^[\\s]*protectKernelDefaults[\\s]*:[\\s]*true"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.6 Ensure that the --protect-kernel-defaults argument is set to true"
              info        : "Protect tuned kernel parameters from overriding kubelet default kernel parameter values.

Rationale:

Kernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior."
              solution    : "If using a Kubelet config file, edit the file to set 'protectKernelDefaults: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--protect-kernel-defaults=true

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.4.2,800-53|CM-6,CSCv6|3,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.7 Ensure that the --make-iptables-util-chains argument is set to true"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--make-iptables-util-chains="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.7 Ensure that the --make-iptables-util-chains argument is set to true"
          info          : "Allow Kubelet to manage iptables.

Rationale:

Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open."
          solution      : "If using a Kubelet config file, edit the file to set 'makeIPTablesUtilChains: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and remove the '--make-iptables-util-chains' argument from the 'KUBELET_SYSTEM_PODS_ARGS' variable.

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-171|3.13.1,800-171|3.13.6,800-53|SC-7(5),CN-L3|7.1.2.2(c),CSCv6|9,CSF|PR.PT-4,ITSG-33|SC-7(5),LEVEL|1S,NIAv2|GS7b,NIAv2|NS25"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--make-iptables-util-chains=true"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.7 Ensure that the --make-iptables-util-chains argument is set to true"
              info        : "Allow Kubelet to manage iptables.

Rationale:

Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open."
              solution    : "If using a Kubelet config file, edit the file to set 'makeIPTablesUtilChains: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and remove the '--make-iptables-util-chains' argument from the 'KUBELET_SYSTEM_PODS_ARGS' variable.

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.13.1,800-171|3.13.6,800-53|SC-7(5),CN-L3|7.1.2.2(c),CSCv6|9,CSF|PR.PT-4,ITSG-33|SC-7(5),LEVEL|1S,NIAv2|GS7b,NIAv2|NS25"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*makeIPTablesUtilChains[\\s]*:"
              expect      : "^[\\s]*makeIPTablesUtilChains[\\s]*:[\\s]*true"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.7 Ensure that the --make-iptables-util-chains argument is set to true"
              info        : "Allow Kubelet to manage iptables.

Rationale:

Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open."
              solution    : "If using a Kubelet config file, edit the file to set 'makeIPTablesUtilChains: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and remove the '--make-iptables-util-chains' argument from the 'KUBELET_SYSTEM_PODS_ARGS' variable.

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.13.1,800-171|3.13.6,800-53|SC-7,CN-L3|7.1.2.2(c),CSCv6|9,CSF|PR.PT-4,ITSG-33|SC-7,LEVEL|1S,NIAv2|GS7b,NIAv2|NS25"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <custom_item>
      system        : "Linux"
      type          : CMD_EXEC
      description   : "2.1.8 Ensure that the --hostname-override argument is not set"
      info          : "Do not override node hostnames.

Rationale:

Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs."
      solution      : "Edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and remove the '--hostname-override' argument from the 'KUBELET_SYSTEM_PODS_ARGS' variable.

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
      reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|3,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
      see_also      : "https://workbench.cisecurity.org/files/2421"
      cmd           : "ps -ef | grep kubelet | grep -v grep"
      expect        : "^((?!--hostname-override).)*$"
      dont_echo_cmd : YES
    </custom_item>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.9 Ensure that the --event-qps argument is set to 0"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--event-qps="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.9 Ensure that the --event-qps argument is set to 0"
          info          : "Do not limit event creation.

Rationale:

It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data."
          solution      : "If using a Kubelet config file, edit the file to set 'eventRecordQPS: 0'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--event-qps=0

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|6,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--event-qps=0([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.9 Ensure that the --event-qps argument is set to 0"
              info        : "Do not limit event creation.

Rationale:

It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data."
              solution    : "If using a Kubelet config file, edit the file to set 'eventRecordQPS: 0'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--event-qps=0

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|6,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*eventRecordQPS[\\s]*:"
              expect      : "^[\\s]*eventRecordQPS[\\s]*:[\\s]*0"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.9 Ensure that the --event-qps argument is set to 0"
              info        : "Do not limit event creation.

Rationale:

It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data."
              solution    : "If using a Kubelet config file, edit the file to set 'eventRecordQPS: 0'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_SYSTEM_PODS_ARGS' variable.

--event-qps=0

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-171|3.4.2,800-53|CM-6,CSCv6|6,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,SWIFT-CSCv1|2.3"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - cert"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--tls-cert-file="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - cert"
          info          : "Setup TLS connection on the Kubelets.

Rationale:

Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic."
          solution      : "If using a Kubelet config file, edit the file to set 'tlsCertFile' to the location of the certificate file to use to identify this Kubelet, and 'tlsPrivateKeyFile' to the location of the corresponding private key file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameters in 'KUBELET_CERTIFICATE_ARGS' variable.

--tls-cert-file=
--tls-private-key-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @TLS_CERT_FILE@ replaced with "/etc/kubernetes/pki/apiserver.crt" in field "expect".
          expect        : "--tls-cert-file=/etc/kubernetes/pki/apiserver.crt([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - cert"
              info        : "Setup TLS connection on the Kubelets.

Rationale:

Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic."
              solution    : "If using a Kubelet config file, edit the file to set 'tlsCertFile' to the location of the certificate file to use to identify this Kubelet, and 'tlsPrivateKeyFile' to the location of the corresponding private key file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameters in 'KUBELET_CERTIFICATE_ARGS' variable.

--tls-cert-file=
--tls-private-key-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*tlsCertFile[\\s]*:"
# Note: Variable @TLS_CERT_FILE@ replaced with "/etc/kubernetes/pki/apiserver.crt" in field "expect".
              expect      : "^[\\s]*tlsCertFile[\\s]*:[\\s]*/etc/kubernetes/pki/apiserver.crt"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - cert"
              info        : "Setup TLS connection on the Kubelets.

Rationale:

Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic."
              solution    : "If using a Kubelet config file, edit the file to set 'tlsCertFile' to the location of the certificate file to use to identify this Kubelet, and 'tlsPrivateKeyFile' to the location of the corresponding private key file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameters in 'KUBELET_CERTIFICATE_ARGS' variable.

--tls-cert-file=
--tls-private-key-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|IA-5,CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5,LEVEL|1S"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - key"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @TLS_PRIVATE_KEY_FILE@ replaced with "/etc/kubernetes/pki/apiserver.key" in field "expect".
          expect      : "--tls-private-key-file=/etc/kubernetes/pki/apiserver.key([\\s]|$)"
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - key"
          info          : "Setup TLS connection on the Kubelets.

Rationale:

Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic."
          solution      : "If using a Kubelet config file, edit the file to set 'tlsCertFile' to the location of the certificate file to use to identify this Kubelet, and 'tlsPrivateKeyFile' to the location of the corresponding private key file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameters in 'KUBELET_CERTIFICATE_ARGS' variable.

--tls-cert-file=
--tls-private-key-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @TLS_PRIVATE_KEY_FILE@ replaced with "/etc/kubernetes/pki/apiserver.key" in field "expect".
          expect        : "--tls-private-key-file=/etc/kubernetes/pki/apiserver.key([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - key"
              info        : "Setup TLS connection on the Kubelets.

Rationale:

Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic."
              solution    : "If using a Kubelet config file, edit the file to set 'tlsCertFile' to the location of the certificate file to use to identify this Kubelet, and 'tlsPrivateKeyFile' to the location of the corresponding private key file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameters in 'KUBELET_CERTIFICATE_ARGS' variable.

--tls-cert-file=
--tls-private-key-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|IA-5(2),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(2),LEVEL|1S"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*tlsPrivateKeyFile[\\s]*:"
# Note: Variable @TLS_PRIVATE_KEY_FILE@ replaced with "/etc/kubernetes/pki/apiserver.key" in field "expect".
              expect      : "^[\\s]*tlsPrivateKeyFile[\\s]*:[\\s]*/etc/kubernetes/pki/apiserver.key"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - key"
              info        : "Setup TLS connection on the Kubelets.

Rationale:

Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic."
              solution    : "If using a Kubelet config file, edit the file to set 'tlsCertFile' to the location of the certificate file to use to identify this Kubelet, and 'tlsPrivateKeyFile' to the location of the corresponding private key file.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameters in 'KUBELET_CERTIFICATE_ARGS' variable.

--tls-cert-file=
--tls-private-key-file=

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|IA-5,CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5,LEVEL|1S"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.12 Ensure that the --rotate-certificates argument is not set to false"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--rotate-certificates=true"
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.12 Ensure that the --rotate-certificates argument is not set to false"
          info          : "Enable kubelet client certificate rotation.

Rationale:

The '--rotate-certificates' setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad.

**Note:** This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself.

**Note:** This feature also require the 'RotateKubeletClientCertificate' feature gate to be enabled (which is the default since Kubernetes v1.7)"
          solution      : "If using a Kubelet config file, edit the file to add the line 'rotateCertificates: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and add '--rotate-certificates=true' argument to the 'KUBELET_CERTIFICATE_ARGS' variable.

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
          reference     : "800-53|CM-3(6),CSCv6|14.2,CSCv7|14.4,CSF|PR.IP-1,CSF|PR.IP-3,ISO/IEC-27001|A.10.1.1,LEVEL|1S"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--rotate-certificates=true"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.12 Ensure that the --rotate-certificates argument is not set to false"
              info        : "Enable kubelet client certificate rotation.

Rationale:

The '--rotate-certificates' setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad.

**Note:** This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself.

**Note:** This feature also require the 'RotateKubeletClientCertificate' feature gate to be enabled (which is the default since Kubernetes v1.7)"
              solution    : "If using a Kubelet config file, edit the file to add the line 'rotateCertificates: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and add '--rotate-certificates=true' argument to the 'KUBELET_CERTIFICATE_ARGS' variable.

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|CM-3(6),CSCv6|14.2,CSCv7|14.4,CSF|PR.IP-1,CSF|PR.IP-3,ISO/IEC-27001|A.10.1.1,LEVEL|1S"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*rotateCertificates[\\s]*:"
              expect      : "^[\\s]*rotateCertificates[\\s]*:[\\s]*true"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.12 Ensure that the --rotate-certificates argument is not set to false"
              info        : "Enable kubelet client certificate rotation.

Rationale:

The '--rotate-certificates' setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad.

**Note:** This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself.

**Note:** This feature also require the 'RotateKubeletClientCertificate' feature gate to be enabled (which is the default since Kubernetes v1.7)"
              solution    : "If using a Kubelet config file, edit the file to add the line 'rotateCertificates: true'.

If using command line arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and add '--rotate-certificates=true' argument to the 'KUBELET_CERTIFICATE_ARGS' variable.

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
              reference   : "800-53|CM-3,CSCv6|14.2,CSCv7|14.4,CSF|PR.IP-1,CSF|PR.IP-3,LEVEL|1S"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <custom_item>
      system        : "Linux"
      type          : CMD_EXEC
      description   : "2.1.13 Ensure that the RotateKubeletServerCertificate argument is set to true"
      info          : "Enable kubelet server certificate rotation.

Rationale:

'RotateKubeletServerCertificate' causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad.

Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself."
      solution      : "Edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter in 'KUBELET_CERTIFICATE_ARGS' variable.

--feature-gates=RotateKubeletServerCertificate=true

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service"
      reference     : "800-53|IA-5(1),CSCv6|14.2,CSCv7|14.4,CSF|PR.AC-1,ITSG-33|IA-5(1),LEVEL|1S,NESA|T5.2.3,SWIFT-CSCv1|4.1"
      see_also      : "https://workbench.cisecurity.org/files/2421"
      cmd           : "ps -ef | grep kubelet | grep -v grep"
      expect        : "RotateKubeletServerCertificate=true"
      dont_echo_cmd : YES
    </custom_item>

    <if>
      <condition type:"AND">
        <custom_item>
          system      : "Linux"
          type        : CMD_EXEC
          description : "2.1.14 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers"
          cmd         : "ps -ef | grep kubelet | grep -v grep"
          expect      : "--tls-cipher-suites="
        </custom_item>
      </condition>

      <then>
        <custom_item>
          system        : "Linux"
          type          : CMD_EXEC
          description   : "2.1.14 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers"
          info          : "Ensure that the Kubelet is configured to only use strong cryptographic ciphers.

Rationale:

TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided."
          solution      : "If using a Kubelet config file, edit the file to set 'TLSCipherSuites:' to 'TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256' or to a subset of these values.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter as follows, or to a subset of these values.

--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service

Impact:

Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.

Default Value:

By default the Kubernetes API server supports a wide range of TLS ciphers"
          reference     : "800-171|3.13.11,800-53|SC-13,CSCv6|3.4,CSCv7|4.5,CSF|PR.DS-5,ISO/IEC-27001|A.10.1.1,ITSG-33|SC-13,LEVEL|1S,NESA|M5.2.6,NESA|T7.4.1,NIAv2|CY3,NIAv2|CY4,NIAv2|CY5b,NIAv2|CY5c,NIAv2|CY5d,NIAv2|CY7,NIAv2|NS5e"
          see_also      : "https://workbench.cisecurity.org/files/2421"
          cmd           : "ps -ef | grep kubelet | grep -v grep"
          expect        : "--tls-cipher-suites=((TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256|TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256|TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305|TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384|TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305|TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384|TLS_RSA_WITH_AES_256_GCM_SHA384|TLS_RSA_WITH_AES_128_GCM_SHA256)[,]?)+([\\s]|$)"
          dont_echo_cmd : YES
        </custom_item>
      </then>

      <else>
        <if>
          <condition type:"AND">
            <custom_item>
              system      : "Linux"
              type        : CMD_EXEC
              description : "Check if Kubelet config file is used"
              cmd         : "ps -ef | grep kubelet | grep -v grep"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "expect".
              expect      : "--config=/var/lib/kubelet/config.yaml([\\s]|$)"
            </custom_item>
          </condition>

          <then>
            <custom_item>
              system      : "Linux"
              type        : FILE_CONTENT_CHECK
              description : "2.1.14 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers"
              info        : "Ensure that the Kubelet is configured to only use strong cryptographic ciphers.

Rationale:

TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided."
              solution    : "If using a Kubelet config file, edit the file to set 'TLSCipherSuites:' to 'TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256' or to a subset of these values.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter as follows, or to a subset of these values.

--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service

Impact:

Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.

Default Value:

By default the Kubernetes API server supports a wide range of TLS ciphers"
              reference   : "800-171|3.13.11,800-53|SC-13,CSCv6|3.4,CSCv7|4.5,CSF|PR.DS-5,ISO/IEC-27001|A.10.1.1,ITSG-33|SC-13,LEVEL|1S,NESA|M5.2.6,NESA|T7.4.1,NIAv2|CY3,NIAv2|CY4,NIAv2|CY5b,NIAv2|CY5c,NIAv2|CY5d,NIAv2|CY7,NIAv2|NS5e"
              see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
              file        : "/var/lib/kubelet/config.yaml"
              regex       : "^[\\s]*TLSCipherSuites[\\s]*:"
              expect      : "^[\\s]*TLSCipherSuites[\\s]*:[\\s]*TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256"
            </custom_item>
          </then>

          <else>
            <report type:"FAILED">
              description : "2.1.14 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers"
              info        : "Ensure that the Kubelet is configured to only use strong cryptographic ciphers.

Rationale:

TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided."
              solution    : "If using a Kubelet config file, edit the file to set 'TLSCipherSuites:' to 'TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256' or to a subset of these values.

If using executable arguments, edit the kubelet service file '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf' on each worker node and set the below parameter as follows, or to a subset of these values.

--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256

Based on your system, restart the 'kubelet' service. For example:

systemctl daemon-reload
systemctl restart kubelet.service

Impact:

Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.

Default Value:

By default the Kubernetes API server supports a wide range of TLS ciphers"
              reference   : "800-171|3.13.11,800-53|SC-13,CSCv6|3.4,CSCv7|4.5,CSF|PR.DS-5,ITSG-33|SC-13,LEVEL|1S,NESA|M5.2.6,NESA|T7.4.1,NIAv2|CY3,NIAv2|CY4,NIAv2|CY5b,NIAv2|CY5c,NIAv2|CY5d,NIAv2|CY7,NIAv2|NS5e"
              see_also    : "https://workbench.cisecurity.org/files/2421"
            </report>
          </else>
        </if>
      </else>
    </if>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.1 Ensure that the kubelet.conf file permissions are set to 644 or more restrictive"
      info        : "Ensure that the 'kubelet.conf' file has permissions of '644' or more restrictive.

Rationale:

The 'kubelet.conf' file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
      solution    : "Run the below command (based on the file location on your system) on the each worker node. For example,

chmod 644 /etc/kubernetes/kubelet.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @CONFIG_FILE@ replaced with "/etc/kubernetes/kubelet.conf" in field "file".
      file        : "/etc/kubernetes/kubelet.conf"
      mask        : "133"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.2 Ensure that the kubelet.conf file ownership is set to root:root"
      info        : "Ensure that the 'kubelet.conf' file ownership is set to 'root:root'.

Rationale:

The 'kubelet.conf' file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
      solution    : "Run the below command (based on the file location on your system) on the each worker node. For example,

chown root:root /etc/kubernetes/kubelet.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @CONFIG_FILE@ replaced with "/etc/kubernetes/kubelet.conf" in field "file".
      file        : "/etc/kubernetes/kubelet.conf"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.3 Ensure that the kubelet service file permissions are set to 644 or more restrictive"
      info        : "Ensure that the 'kubelet' service file has permissions of '644' or more restrictive.

Rationale:

The 'kubelet' service file controls various parameters that set the behavior of the 'kubelet' service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
      solution    : "Run the below command (based on the file location on your system) on the each worker node. For example,

chmod 755 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_FILE@ replaced with "/etc/systemd/system/kubelet.service.d/10-kubeadm.conf" in field "file".
      file        : "/etc/systemd/system/kubelet.service.d/10-kubeadm.conf"
      mask        : "133"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.4 Ensure that the kubelet service file ownership is set to root:root"
      info        : "Ensure that the 'kubelet' service file ownership is set to 'root:root'.

Rationale:

The 'kubelet' service file controls various parameters that set the behavior of the 'kubelet' service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
      solution    : "Run the below command (based on the file location on your system) on the each worker node. For example,

chown root:root /etc/systemd/system/kubelet.service.d/10-kubeadm.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_FILE@ replaced with "/etc/systemd/system/kubelet.service.d/10-kubeadm.conf" in field "file".
      file        : "/etc/systemd/system/kubelet.service.d/10-kubeadm.conf"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.5 Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive"
      info        : "If 'kube-proxy' is running, ensure that the proxy kubeconfig file has permissions of '644' or more restrictive.

Rationale:

The 'kube-proxy' kubeconfig file controls various parameters of the 'kube-proxy' service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
      solution    : "Run the below command (based on the file location on your system) on the each worker node. For example,

chmod 644"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @PROXY_FILE@ replaced with "/etc/kubernetes/proxy" in field "file".
      file        : "/etc/kubernetes/proxy"
      mask        : "133"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.6 Ensure that the proxy kubeconfig file ownership is set to root:root"
      info        : "If 'kube-proxy' is running, ensure that the file ownership of its kubeconfig file is set to 'root:root'.

Rationale:

The kubeconfig file for 'kube-proxy' controls various parameters for the 'kube-proxy' service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
      solution    : "Run the below command (based on the file location on your system) on the each worker node. For example,

chown root:root"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @PROXY_FILE@ replaced with "/etc/kubernetes/proxy" in field "file".
      file        : "/etc/kubernetes/proxy"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.7 Ensure that the certificate authorities file permissions are set to 644 or more restrictive"
      info        : "Ensure that the certificate authorities file has permissions of '644' or more restrictive.

Rationale:

The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
      solution    : "Run the following command to modify the file permissions of the '--client-ca-file'

chmod 644"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|14.4,CSCv6|5.1,CSCv7|14.6,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @CLIENT_CA_FILE@ replaced with "/etc/kubernetes/pki/ca.crt" in field "file".
      file        : "/etc/kubernetes/pki/ca.crt"
      mask        : "133"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.8 Ensure that the client certificate authorities file ownership is set to root:root"
      info        : "Ensure that the certificate authorities file ownership is set to root:root.

Rationale:

The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by 'root:root'."
      solution    : "Run the following command to modify the ownership of the '--client-ca-file'.

chown root:root"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @CLIENT_CA_FILE@ replaced with "/etc/kubernetes/pki/ca.crt" in field "file".
      file        : "/etc/kubernetes/pki/ca.crt"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.9 Ensure that the kubelet configuration file ownership is set to root:root"
      info        : "Ensure that if the kubelet refers to a configuration file with the '--config' argument, that file is owned by root:root.

Rationale:

The kubelet reads various parameters, including security settings, from a config file specified by the '--config' argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root."
      solution    : "Run the following command (using the config file location identied in the Audit step)

chown root:root /etc/kubernetes/kubelet.conf"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
      file        : "/var/lib/kubelet/config.yaml"
      owner       : "root"
      group       : "root"
    </custom_item>

    <custom_item>
      system      : "Linux"
      type        : FILE_CHECK
      description : "2.2.10 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive"
      info        : "Ensure that if the kubelet refers to a configuration file with the '--config' argument, that file has permissions of 644 or more restrictive.

Rationale:

The kubelet reads various parameters, including security settings, from a config file specified by the '--config' argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system."
      solution    : "Run the following command (using the config file location identied in the Audit step)

chmod 644 /var/lib/kubelet/config.yaml"
      reference   : "800-171|3.4.2,800-53|CM-6,CN-L3|8.1.10.6(d),CSCv6|5.1,CSF|PR.IP-1,ITSG-33|CM-6,LEVEL|1S,NESA|T3.2.1,PCI-DSSv3.1|2.2.4,PCI-DSSv3.2|2.2.4,SWIFT-CSCv1|2.3"
      see_also    : "https://workbench.cisecurity.org/files/2421"
# Note: Variable @KUBELET_CONFIG_FILE@ replaced with "/var/lib/kubelet/config.yaml" in field "file".
      file        : "/var/lib/kubelet/config.yaml"
      mask        : "133"
    </custom_item>
  </then>
</if>

</check_type>
